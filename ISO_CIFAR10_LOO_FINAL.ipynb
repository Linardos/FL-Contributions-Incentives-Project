{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ISO CIFAR10 LOO FINAL",
      "provenance": [],
      "authorship_tag": "ABX9TyNy9xk0Wtb0L4gk62WPYqHa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vs-152/FL-Contributions-Incentives-Project/blob/main/ISO_CIFAR10_LOO_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgwtkMkH37Cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ab94fd-ca81-4c05-c938-c409b6913654"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "!pip install pulp\n",
        "import pulp\n",
        "import copy\n",
        "import time\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from itertools import chain, combinations\n",
        "from tqdm import tqdm\n",
        "from scipy.special import comb\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pulp in /usr/local/lib/python3.7/dist-packages (2.4)\n",
            "Requirement already satisfied: amply>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from pulp) (0.1.4)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.7/dist-packages (from amply>=0.1.2->pulp) (0.17)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from amply>=0.1.2->pulp) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5hHwk9S3-zy"
      },
      "source": [
        "def print_solution(model):\n",
        "    \"\"\"Prints solution of the model nicely!\"\"\"\n",
        "\n",
        "    print(f\"status: {model.status}, {pulp.LpStatus[model.status]}\")\n",
        "    print(f\"objective: {model.objective.value()}\")\n",
        "    for var in model.variables():\n",
        "        print(f\"{var.name}: {round(var.value(),3)}\")\n",
        "\n",
        "def noisify_MNIST(noise_rate, noise_type, x, y, perm=[], **kwargs):\n",
        "    '''Returns a symmetrically noisy dataset\n",
        "    or a an asymmetrically noisy dataset with permutation matrix perm.\n",
        "    '''\n",
        "    if (noise_rate == 0.):\n",
        "        return y, []\n",
        "    if 'seed' in kwargs:\n",
        "        _, noise_idx = next(\n",
        "            iter(StratifiedShuffleSplit(\n",
        "                n_splits=1,\n",
        "                test_size=noise_rate,\n",
        "                random_state=kwargs['seed']).split(x, y)))\n",
        "    else:\n",
        "        _, noise_idx = next(iter(StratifiedShuffleSplit(\n",
        "            n_splits=1, test_size=noise_rate).split(x, y)))\n",
        "    y_noisy = y.copy()\n",
        "    if (noise_type == 'symmetric'):\n",
        "        for i in noise_idx:\n",
        "            t1 = np.arange(10)\n",
        "            t2 = np.delete(t1, y[i])\n",
        "            y_noisy[i] = np.random.choice(t2, 1)\n",
        "    elif (noise_type == 'asymmetric'):\n",
        "        pure_noise = perm[y]\n",
        "        for i in noise_idx:\n",
        "            if (perm[y[i]] == y[i]):\n",
        "                noise_idx = np.delete(noise_idx, np.where(noise_idx == i))\n",
        "            else:\n",
        "                y_noisy[i] = pure_noise[i]\n",
        "\n",
        "    return y_noisy, noise_idx\n",
        "\n",
        "def mnist_iid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample I.I.D. client data from MNIST dataset\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :return: dict of image index\n",
        "    \"\"\"\n",
        "    num_items = int(len(dataset)/num_users)\n",
        "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
        "    for i in range(num_users):\n",
        "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
        "                                             replace=False))\n",
        "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
        "\n",
        "    return dict_users\n",
        "\n",
        "def average_weights(w, fraction):  # this can also be used to average gradients\n",
        "    \"\"\"\n",
        "    :param w: list of weights generated from the users\n",
        "    :param fraction: list of fraction of data from the users\n",
        "    :Returns the weighted average of the weights.\n",
        "    \"\"\"\n",
        "    w_avg = copy.deepcopy(w[0]) #copy the weights from the first user in the list \n",
        "    for key in w_avg.keys():\n",
        "        w_avg[key] *= torch.tensor(fraction[0]/sum(fraction), dtype=w_avg[key].dtype)\n",
        "        for i in range(1, len(w)):\n",
        "            w_avg[key] += w[i][key] * torch.tensor(fraction[0]/sum(fraction), dtype=w_avg[key].dtype)\n",
        "\n",
        "    return w_avg\n",
        "\n",
        "def calculate_gradients(new_weights, old_weights):\n",
        "    \"\"\"\n",
        "    :param new_weights: list of weights generated from the users\n",
        "    :param old_weights: old weights of a model, probably before training\n",
        "    :Returns the list of gradients.\n",
        "    \"\"\"\n",
        "    gradients = []\n",
        "    for i in range(len(new_weights)):\n",
        "        gradients.append(copy.deepcopy(new_weights[i]))\n",
        "        for key in gradients[i].keys():\n",
        "            gradients[i][key] -= old_weights[key]\n",
        "\n",
        "    return gradients\n",
        "\n",
        "def update_weights_from_gradients(gradients, old_weights):\n",
        "    \"\"\"\n",
        "    :param gradients: gradients\n",
        "    :param old_weights: old weights of a model, probably before training\n",
        "    :Returns the updated weights calculated by: old_weights+gradients.\n",
        "    \"\"\"\n",
        "    updated_weights = copy.deepcopy(old_weights)\n",
        "    for key in updated_weights.keys():\n",
        "        updated_weights[key] = old_weights[key] + gradients[key]\n",
        "\n",
        "    return updated_weights\n",
        "    \n",
        "\n",
        "\n",
        "def powersettool(iterable):\n",
        "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
        "    s = list(iterable)\n",
        "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
        "\n",
        "def least_core(char_function_dict, N):\n",
        "    \"\"\"Solves the least core LP problem.\n",
        "\n",
        "    Args:\n",
        "        N: number of participants.\n",
        "        char_function_dict: dictionary with participants as keys and \n",
        "        corresponding characteristic function value as values\n",
        "    \"\"\"\n",
        "    model = pulp.LpProblem('least_core', pulp.LpMinimize)\n",
        "    x = {i: pulp.LpVariable(name=f'x({i})', lowBound=0) for i in range(1, N+1)}\n",
        "    e = pulp.LpVariable(name='e')\n",
        "    model += e # decision variable\n",
        "    grand_coalition = tuple(i for i in range(1, N+1))\n",
        "    model += pulp.lpSum(x) == char_function_dict[grand_coalition]\n",
        "    for key, value in char_function_dict.items():\n",
        "        model += pulp.lpSum(x[idx] for idx in key) + e >= value\n",
        "    model.solve()\n",
        "    print_solution(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "def shapley(utility, N):\n",
        "\n",
        "    shapley_dict = {}\n",
        "    for i in range(1, N+1):\n",
        "        shapley_dict[i] = 0\n",
        "    for key in utility:\n",
        "        if key != ():\n",
        "            for contributor in key:\n",
        "                # print('contributor:', contributor, key) # print check\n",
        "                marginal_contribution = utility[key] - utility[tuple(i for i in key if i!=contributor)]\n",
        "                # print('marginal:', marginal_contribution) # print check\n",
        "                shapley_dict[contributor] += marginal_contribution /((comb(N-1,len(key)-1))*N)\n",
        "\n",
        "    return shapley_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xg-nE1b3w-f",
        "outputId": "e2ac2a0b-cf5e-4113-ca09-2d5c51d845df"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    #transforms.RandomErasing(scale=(0.1, 0.3), ratio=(0.5, 2), value=0)\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = CIFAR10(\n",
        "    root='./data', train=True, download=True)\n",
        "\n",
        "test_dataset = CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "x_train = trainset.data\n",
        "y_train = np.array(trainset.targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrrjBXZg4rEN"
      },
      "source": [
        "class ResNet9(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet9, self).__init__()\n",
        "        self.prep = self.convbnrelu(channels=3, filters=64)\n",
        "        self.layer1 = self.convbnrelu(64, 128)\n",
        "        self.layer_pool = nn.MaxPool2d(2, 2, 0, 1, ceil_mode=False)\n",
        "        self.layer1r1 = self.convbnrelu(128, 128)\n",
        "        self.layer1r2 = self.convbnrelu(128, 128)\n",
        "        self.layer2 = self.convbnrelu(128, 256)\n",
        "        self.layer3 = self.convbnrelu(256, 512)\n",
        "        self.layer3r1 = self.convbnrelu(512, 512)\n",
        "        self.layer3r2 = self.convbnrelu(512, 512)\n",
        "        self.out_pool = nn.MaxPool2d(kernel_size=4, stride=4, ceil_mode=False)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(in_features=512, out_features=10, bias=False)\n",
        "\n",
        "    def convbnrelu(self, channels, filters):\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(channels, filters, (3, 3),\n",
        "                                (1, 1), (1, 1), bias=False))\n",
        "        layers.append(nn.BatchNorm2d(filters, track_running_stats=False))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.prep(x)\n",
        "        x = self.layer_pool(self.layer1(x))\n",
        "        r1 = self.layer1r2(self.layer1r1(x)) \n",
        "        x = x + r1\n",
        "        x = self.layer_pool(self.layer2(x))\n",
        "        x = self.layer_pool(self.layer3(x))\n",
        "        r3 = self.layer3r2(self.layer3r1(x))\n",
        "        x = x + r3\n",
        "        out = self.out_pool(x)\n",
        "        out = self.flatten(out)\n",
        "        out = self.linear(out)\n",
        "        out = out * 0.125\n",
        "\n",
        "        return out\n",
        "\n",
        "class CustomTensorDataset(Dataset):\n",
        "    \"\"\"TensorDataset with support of transforms.\n",
        "    \"\"\"\n",
        "    def __init__(self, tensors, transform=None):\n",
        "        self.tensors = tensors\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.tensors[0][index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        y = self.tensors[1][index]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tensors[0].shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_xb9tLd32A6"
      },
      "source": [
        "class LocalUpdate(object):\n",
        "\n",
        "    def __init__(self, lr, local_ep, trainloader):\n",
        "        self.lr = lr\n",
        "        self.local_ep = local_ep\n",
        "        self.trainloader = trainloader\n",
        "\n",
        "    def update_weights(self, model):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = []\n",
        "        optimizer = torch.optim.Adam(model.parameters())\n",
        "        criterion = nn.CrossEntropyLoss().to(device)\n",
        "        for iter in range(self.local_ep):\n",
        "            batch_loss = []\n",
        "            for batch_idx, (images, labels) in enumerate(self.trainloader):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                model.zero_grad()   \n",
        "                log_probs = model(images)\n",
        "                loss = criterion(log_probs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                batch_loss.append(loss.item())\n",
        "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
        "\n",
        "        return model.state_dict(), sum(epoch_loss) / len(epoch_loss)\n",
        "\n",
        "def test_inference(model, test_dataset):\n",
        "\n",
        "    model.eval()\n",
        "    loss, total, correct = 0.0, 0.0, 0.0\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    testloader = DataLoader(test_dataset, batch_size=200, shuffle=False)\n",
        "\n",
        "    for _, (images, labels) in enumerate(testloader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        loss += batch_loss.item()\n",
        "        _, pred_labels = torch.max(outputs, 1)\n",
        "        pred_labels = pred_labels.view(-1)\n",
        "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "        total += len(labels)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return accuracy, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTUtuKlu4Ddo"
      },
      "source": [
        "N = 10 #srch\n",
        "local_bs = 512\n",
        "lr = 0.01\n",
        "local_ep = 5\n",
        "EPOCHS = 5\n",
        "\n",
        "noise_rates = np.linspace(0, 1, N, endpoint=False)\n",
        "split_dset = mnist_iid(trainset, N)\n",
        "user_groups = {i: 0 for i in range(1, N+1)}\n",
        "noise_idx = {i: 0 for i in range(1, N+1)}\n",
        "train_datasets = {i: 0 for i in range(1, N+1)}\n",
        "for n in range(N):\n",
        "    user_groups[n+1] = np.array(list(split_dset[n]), dtype=np.int)\n",
        "    user_train_x, user_train_y = x_train[user_groups[n+1]], y_train[user_groups[n+1]]\n",
        "    user_noisy_y, noise_idx[n+1] = noisify_MNIST(noise_rates[n], 'symmetric', user_train_x, user_train_y)\n",
        "    \n",
        "    train_datasets[n+1] = CustomTensorDataset((user_train_x, user_noisy_y), transform_train)\n",
        "\n",
        "def fixfuckingbn(subset_weights, global_model_state_dict):\n",
        "    for pair_1, pair_2 in zip(subset_weights.items(), global_model_state_dict.items()):\n",
        "        if ('running' in pair_1[0]) or ('batches' in pair_1[0]):\n",
        "            subset_weights[pair_1[0]] = global_model_state_dict[pair_1[0]]\n",
        "    \n",
        "    return subset_weights\n",
        "\n",
        "global_model = ResNet9().to(device)\n",
        "global_model.to(device)\n",
        "global_model.train()\n",
        "\n",
        "global_weights = global_model.state_dict()\n",
        "powerset = list(powersettool(range(1, N+1)))\n",
        "\n",
        "loo_sets = list(i for i in powersettool(range(1, N+1)) if len(i)==N-1)\n",
        "submodel_dict = {}  \n",
        "submodel_dict[()] = copy.deepcopy(global_model)\n",
        "accuracy_dict = {}\n",
        "loo_array = np.zeros((EPOCHS, N))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvXdFSu24Iq6",
        "outputId": "cc452d85-ca1e-424f-8dfa-b474de3da519"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for subset in loo_sets:\n",
        "    submodel_dict[subset] = copy.deepcopy(global_model)\n",
        "    submodel_dict[subset].to(device)\n",
        "    submodel_dict[subset].train()\n",
        " \n",
        "train_loss, train_accuracy = [], []\n",
        "val_acc_list, net_list = [], []\n",
        "print_every = 1\n",
        "\n",
        "idxs_users = np.arange(1, N+1)\n",
        "total_data = sum(len(user_groups[i]) for i in range(1, N+1))\n",
        "fraction = [len(user_groups[i])/total_data for i in range(1, N+1)]\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    local_weights, local_losses = [], []\n",
        "    print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
        "    global_model.train()\n",
        "    for idx in idxs_users:\n",
        "        trainloader = DataLoader(train_datasets[idx], batch_size=local_bs, shuffle=True)\n",
        "        local_model = LocalUpdate(lr, local_ep, trainloader)\n",
        "        w, loss = local_model.update_weights(model=copy.deepcopy(global_model))\n",
        "        local_weights.append(copy.deepcopy(w))\n",
        "        local_losses.append(copy.deepcopy(loss))\n",
        "    global_weights = average_weights(local_weights, fraction) \n",
        "    loss_avg = sum(local_losses) / len(local_losses)\n",
        "    train_loss.append(loss_avg)\n",
        "\n",
        "    gradients = calculate_gradients(local_weights, global_model.state_dict()) \n",
        "    for subset in loo_sets: \n",
        "        subset_gradient = average_weights([gradients[i-1] for i in subset], [fraction[i-1] for i in subset])\n",
        "        subset_weights = update_weights_from_gradients(subset_gradient, submodel_dict[subset].state_dict())\n",
        "        subset_weights = fixfuckingbn(subset_weights, global_model.state_dict())\n",
        "        submodel_dict[subset].load_state_dict(subset_weights)\n",
        "\n",
        "    global_model.load_state_dict(global_weights)\n",
        "    global_model.eval()\n",
        "\n",
        "    if (epoch+1) % print_every == 0:\n",
        "        print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
        "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
        "        # print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
        "\n",
        "    accuracy_dict[powerset[-1]] = test_inference(global_model, test_dataset)[0]\n",
        "\n",
        "        # Test inference for the sub-models in submodel_dict\n",
        "    for subset in loo_sets: \n",
        "        test_acc, test_loss = test_inference(submodel_dict[subset], test_dataset)\n",
        "        print(f' \\n Results after {epoch} global rounds of training:')\n",
        "        print(\"|---- Test Accuracy for {}: {:.2f}%\".format(subset, 100*test_acc))\n",
        "        accuracy_dict[subset] = test_acc\n",
        "        for i in idxs_users:\n",
        "            if i not in subset:\n",
        "                print(i, subset)\n",
        "                loo_array[epoch, i-1] = accuracy_dict[powerset[-1]] - test_acc\n",
        "\n",
        "test_acc, test_loss = test_inference(global_model, test_dataset)\n",
        "print(f' \\n Results after {EPOCHS} global rounds of training:')\n",
        "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
        "\n",
        "accuracy_dict[powerset[-1]] = test_acc\n",
        "\n",
        "\n",
        "trainTime = time.time() - start_time\n",
        "\n",
        "print(f'\\n ACCURACY: {accuracy_dict[powerset[-1]]}')\n",
        "print('\\n Total Time : {0:0.4f}'.format(trainTime))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " | Global Training Round : 1 |\n",
            "\n",
            " \n",
            "Avg Training Stats after 1 global rounds:\n",
            "Training Loss : 2.048456450223923\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 9): 43.58%\n",
            "10 (1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 10): 43.90%\n",
            "9 (1, 2, 3, 4, 5, 6, 7, 8, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 9, 10): 43.38%\n",
            "8 (1, 2, 3, 4, 5, 6, 7, 9, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 8, 9, 10): 43.68%\n",
            "7 (1, 2, 3, 4, 5, 6, 8, 9, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 7, 8, 9, 10): 43.84%\n",
            "6 (1, 2, 3, 4, 5, 7, 8, 9, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 6, 7, 8, 9, 10): 42.47%\n",
            "5 (1, 2, 3, 4, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 5, 6, 7, 8, 9, 10): 42.75%\n",
            "4 (1, 2, 3, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 4, 5, 6, 7, 8, 9, 10): 41.95%\n",
            "3 (1, 2, 4, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 3, 4, 5, 6, 7, 8, 9, 10): 42.77%\n",
            "2 (1, 3, 4, 5, 6, 7, 8, 9, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [02:42<10:50, 162.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (2, 3, 4, 5, 6, 7, 8, 9, 10): 41.82%\n",
            "1 (2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
            "\n",
            " | Global Training Round : 2 |\n",
            "\n",
            " \n",
            "Avg Training Stats after 2 global rounds:\n",
            "Training Loss : 2.013070376992226\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 9): 49.51%\n",
            "10 (1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 10): 49.17%\n",
            "9 (1, 2, 3, 4, 5, 6, 7, 8, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 9, 10): 47.55%\n",
            "8 (1, 2, 3, 4, 5, 6, 7, 9, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 8, 9, 10): 45.88%\n",
            "7 (1, 2, 3, 4, 5, 6, 8, 9, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 7, 8, 9, 10): 47.89%\n",
            "6 (1, 2, 3, 4, 5, 7, 8, 9, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 6, 7, 8, 9, 10): 45.31%\n",
            "5 (1, 2, 3, 4, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 5, 6, 7, 8, 9, 10): 44.68%\n",
            "4 (1, 2, 3, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 4, 5, 6, 7, 8, 9, 10): 45.37%\n",
            "3 (1, 2, 4, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 3, 4, 5, 6, 7, 8, 9, 10): 45.14%\n",
            "2 (1, 3, 4, 5, 6, 7, 8, 9, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [05:25<08:08, 162.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (2, 3, 4, 5, 6, 7, 8, 9, 10): 45.44%\n",
            "1 (2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
            "\n",
            " | Global Training Round : 3 |\n",
            "\n",
            " \n",
            "Avg Training Stats after 3 global rounds:\n",
            "Training Loss : 1.9695328414440156\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 9): 58.22%\n",
            "10 (1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 10): 58.19%\n",
            "9 (1, 2, 3, 4, 5, 6, 7, 8, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 9, 10): 55.22%\n",
            "8 (1, 2, 3, 4, 5, 6, 7, 9, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 8, 9, 10): 53.89%\n",
            "7 (1, 2, 3, 4, 5, 6, 8, 9, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 7, 8, 9, 10): 54.60%\n",
            "6 (1, 2, 3, 4, 5, 7, 8, 9, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 6, 7, 8, 9, 10): 52.45%\n",
            "5 (1, 2, 3, 4, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 5, 6, 7, 8, 9, 10): 52.46%\n",
            "4 (1, 2, 3, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 4, 5, 6, 7, 8, 9, 10): 52.50%\n",
            "3 (1, 2, 4, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 3, 4, 5, 6, 7, 8, 9, 10): 52.31%\n",
            "2 (1, 3, 4, 5, 6, 7, 8, 9, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [08:08<05:25, 162.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (2, 3, 4, 5, 6, 7, 8, 9, 10): 52.19%\n",
            "1 (2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
            "\n",
            " | Global Training Round : 4 |\n",
            "\n",
            " \n",
            "Avg Training Stats after 4 global rounds:\n",
            "Training Loss : 1.9301905167996884\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 9): 62.24%\n",
            "10 (1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 10): 61.88%\n",
            "9 (1, 2, 3, 4, 5, 6, 7, 8, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 9, 10): 59.75%\n",
            "8 (1, 2, 3, 4, 5, 6, 7, 9, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 8, 9, 10): 57.33%\n",
            "7 (1, 2, 3, 4, 5, 6, 8, 9, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 7, 8, 9, 10): 59.66%\n",
            "6 (1, 2, 3, 4, 5, 7, 8, 9, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 6, 7, 8, 9, 10): 57.31%\n",
            "5 (1, 2, 3, 4, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 5, 6, 7, 8, 9, 10): 56.49%\n",
            "4 (1, 2, 3, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 4, 5, 6, 7, 8, 9, 10): 57.06%\n",
            "3 (1, 2, 4, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 3, 4, 5, 6, 7, 8, 9, 10): 56.26%\n",
            "2 (1, 3, 4, 5, 6, 7, 8, 9, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [10:52<02:43, 163.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (2, 3, 4, 5, 6, 7, 8, 9, 10): 56.67%\n",
            "1 (2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
            "\n",
            " | Global Training Round : 5 |\n",
            "\n",
            " \n",
            "Avg Training Stats after 5 global rounds:\n",
            "Training Loss : 1.8962589448690417\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 9): 64.70%\n",
            "10 (1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 10): 66.03%\n",
            "9 (1, 2, 3, 4, 5, 6, 7, 8, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 9, 10): 63.42%\n",
            "8 (1, 2, 3, 4, 5, 6, 7, 9, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 8, 9, 10): 62.28%\n",
            "7 (1, 2, 3, 4, 5, 6, 8, 9, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 7, 8, 9, 10): 62.28%\n",
            "6 (1, 2, 3, 4, 5, 7, 8, 9, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 6, 7, 8, 9, 10): 61.98%\n",
            "5 (1, 2, 3, 4, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 5, 6, 7, 8, 9, 10): 60.55%\n",
            "4 (1, 2, 3, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 4, 5, 6, 7, 8, 9, 10): 60.63%\n",
            "3 (1, 2, 4, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 3, 4, 5, 6, 7, 8, 9, 10): 59.19%\n",
            "2 (1, 3, 4, 5, 6, 7, 8, 9, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [13:35<00:00, 163.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (2, 3, 4, 5, 6, 7, 8, 9, 10): 58.69%\n",
            "1 (2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Results after 5 global rounds of training:\n",
            "|---- Test Accuracy: 65.41%\n",
            "\n",
            " ACCURACY: 0.6541\n",
            "\n",
            " Total Time : 819.2373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOBqzOdQRfIe"
      },
      "source": [
        "def stats(vector):\n",
        "    n = len(vector)\n",
        "    egal = np.array([1/n for i in range(n)])\n",
        "    normalised = np.array(vector / vector.sum())\n",
        "    msg = f'Original vector: {vector}\\n'\n",
        "    msg += f'Normalised vector: {normalised}\\n'\n",
        "    msg += f'Max Dif: {normalised.max()-normalised.min()}\\n'\n",
        "    msg += f'Distance: {np.linalg.norm(normalised-egal)}\\n'\n",
        "\n",
        "    msg += f'Budget: {vector.sum()}\\n'\n",
        "    print(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWyxrrCUwOxO",
        "outputId": "993e1322-4c13-4e0b-c60f-fc8c4ec45932"
      },
      "source": [
        "# STRAIGHT LOO\n",
        "stats(np.array(loo_array.sum(0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original vector: [ 0.1624  0.1538  0.1354  0.1412  0.1153  0.0278  0.0799  0.0173 -0.0812\n",
            " -0.072 ]\n",
            "Normalised vector: [ 0.23885866  0.22620974  0.19914693  0.2076776   0.16958376  0.04088837\n",
            "  0.11751728  0.02544492 -0.11942933 -0.10589793]\n",
            "Max Dif: 0.35828798352698954\n",
            "Distance: 0.4017210354293261\n",
            "Budget: 0.6798999999999994\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ3ZJVjFJA1c",
        "outputId": "02762f41-d6ae-4d56-adab-42b56b54a212"
      },
      "source": [
        "# WEIGHTED LOO\n",
        "base = np.arange(1, EPOCHS+1)\n",
        "stats((np.tile(base,(N,1)).T*loo_array).sum(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original vector: [ 0.6212  0.6055  0.4994  0.5332  0.4194  0.1806  0.3369  0.1128 -0.2296\n",
            " -0.182 ]\n",
            "Normalised vector: [ 0.21439912  0.20898047  0.17236143  0.18402706  0.14475047  0.06233175\n",
            "  0.11627666  0.03893146 -0.07924346 -0.06281494]\n",
            "Max Dif: 0.29364257610271305\n",
            "Distance: 0.3214239296403874\n",
            "Budget: 2.8973999999999975\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55_x4teOJoPc",
        "outputId": "b032b78c-8f4b-476a-c74d-214c6c78080b"
      },
      "source": [
        "# REPUTATION\n",
        "\n",
        "stats(np.heaviside(loo_array, 1).mean(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original vector: [1.  1.  1.  1.  1.  0.6 0.8 0.4 0.  0.2]\n",
            "Normalised vector: [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.08571429\n",
            " 0.11428571 0.05714286 0.         0.02857143]\n",
            "Max Dif: 0.14285714285714285\n",
            "Distance: 0.16288220358559113\n",
            "Budget: 7.000000000000001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}