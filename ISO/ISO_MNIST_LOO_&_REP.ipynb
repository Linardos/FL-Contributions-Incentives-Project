{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ISO MNIST LOO & REP",
      "provenance": [],
      "authorship_tag": "ABX9TyMl+mtZ5ZmQFBQYKNziwJrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vs-152/FL-Contributions-Incentives-Project/blob/main/ISO_MNIST_LOO_%26_REP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsCpQoPb2lAS"
      },
      "source": [
        "%%capture\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "!pip install puLP\n",
        "import pulp\n",
        "import copy\n",
        "import time\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from itertools import chain, combinations\n",
        "from tqdm import tqdm\n",
        "from scipy.special import comb\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    numpy.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEyDKCXj2mLY"
      },
      "source": [
        "def noisify_MNIST(noise_rate, noise_type, x, y, perm=[], **kwargs):\n",
        "    '''Returns a symmetrically noisy dataset\n",
        "    or a an asymmetrically noisy dataset with permutation matrix perm.\n",
        "    '''\n",
        "    if (noise_rate == 0.):\n",
        "        return y, []\n",
        "    if 'seed' in kwargs:\n",
        "        _, noise_idx = next(\n",
        "            iter(StratifiedShuffleSplit(\n",
        "                n_splits=1,\n",
        "                test_size=noise_rate,\n",
        "                random_state=kwargs['seed']).split(x, y)))\n",
        "    else:\n",
        "        _, noise_idx = next(iter(StratifiedShuffleSplit(\n",
        "            n_splits=1, test_size=noise_rate).split(x, y)))\n",
        "    y_noisy = y.copy()\n",
        "    if (noise_type == 'symmetric'):\n",
        "        for i in noise_idx:\n",
        "            t1 = np.arange(10)\n",
        "            t2 = np.delete(t1, y[i])\n",
        "            y_noisy[i] = np.random.choice(t2, 1)\n",
        "    elif (noise_type == 'asymmetric'):\n",
        "        pure_noise = perm[y]\n",
        "        for i in noise_idx:\n",
        "            if (perm[y[i]] == y[i]):\n",
        "                noise_idx = np.delete(noise_idx, np.where(noise_idx == i))\n",
        "            else:\n",
        "                y_noisy[i] = pure_noise[i]\n",
        "\n",
        "    return y_noisy, noise_idx\n",
        "\n",
        "def mnist_iid(dataset, num_users, SEED):\n",
        "    \"\"\"\n",
        "    Sample I.I.D. client data from MNIST dataset\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :return: dict of image index\n",
        "    \"\"\"\n",
        "    np.random.seed(SEED)\n",
        "    num_items = int(len(dataset)/num_users)\n",
        "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
        "    for i in range(num_users):\n",
        "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
        "                                             replace=False))\n",
        "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
        "\n",
        "    return dict_users\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim_in, dim_hidden, dim_out, SEED):\n",
        "        torch.manual_seed(SEED)\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer_input = nn.Linear(dim_in, dim_hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.layer_hidden = nn.Linear(dim_hidden, dim_out)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = self.layer_input(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer_hidden(x)\n",
        "\n",
        "        return self.softmax(x)\n",
        "\n",
        "def average_weights(w, fraction):  # this can also be used to average gradients\n",
        "    \"\"\"\n",
        "    :param w: list of weights generated from the users\n",
        "    :param fraction: list of fraction of data from the users\n",
        "    :Returns the weighted average of the weights.\n",
        "    \"\"\"\n",
        "    w_avg = copy.deepcopy(w[0]) #copy the weights from the first user in the list \n",
        "    for key in w_avg.keys():\n",
        "        w_avg[key] *= (fraction[0]/sum(fraction))\n",
        "        for i in range(1, len(w)):\n",
        "            w_avg[key] += w[i][key] * (fraction[i]/sum(fraction))\n",
        "\n",
        "    return w_avg\n",
        "\n",
        "def calculate_gradients(new_weights, old_weights):\n",
        "    \"\"\"\n",
        "    :param new_weights: list of weights generated from the users\n",
        "    :param old_weights: old weights of a model, probably before training\n",
        "    :Returns the list of gradients.\n",
        "    \"\"\"\n",
        "    gradients = []\n",
        "    for i in range(len(new_weights)):\n",
        "        gradients.append(copy.deepcopy(new_weights[i]))\n",
        "        for key in gradients[i].keys():\n",
        "            gradients[i][key] -= old_weights[key]\n",
        "\n",
        "    return gradients\n",
        "\n",
        "def update_weights_from_gradients(gradients, old_weights):\n",
        "    \"\"\"\n",
        "    :param gradients: gradients\n",
        "    :param old_weights: old weights of a model, probably before training\n",
        "    :Returns the updated weights calculated by: old_weights+gradients.\n",
        "    \"\"\"\n",
        "    updated_weights = copy.deepcopy(old_weights)\n",
        "    for key in updated_weights.keys():\n",
        "        updated_weights[key] = old_weights[key] + gradients[key]\n",
        "\n",
        "    return updated_weights\n",
        "    \n",
        "\n",
        "\n",
        "def powersettool(iterable):\n",
        "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
        "    s = list(iterable)\n",
        "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
        "\n",
        "def shapley(utility, N):\n",
        "\n",
        "    shapley_dict = {}\n",
        "    for i in range(1, N+1):\n",
        "        shapley_dict[i] = 0\n",
        "    for key in utility:\n",
        "        if key != ():\n",
        "            for contributor in key:\n",
        "                # print('contributor:', contributor, key) # print check\n",
        "                marginal_contribution = utility[key] - utility[tuple(i for i in key if i!=contributor)]\n",
        "                # print('marginal:', marginal_contribution) # print check\n",
        "                shapley_dict[contributor] += marginal_contribution /((comb(N-1,len(key)-1))*N)\n",
        "\n",
        "    return shapley_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEKlYGu72m8a"
      },
      "source": [
        "trainset = MNIST(root='./data', train=True, download=True)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "x_train = trainset.data.numpy().astype(\"float32\") / 255.\n",
        "y_train = trainset.targets.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKIVlVT22nnw"
      },
      "source": [
        "class LocalUpdate(object):\n",
        "\n",
        "    def __init__(self, lr, local_ep, trainloader):\n",
        "        self.lr = lr\n",
        "        self.local_ep = local_ep\n",
        "        self.trainloader = trainloader\n",
        "\n",
        "    def update_weights(self, model):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = []\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=self.lr, momentum=0.5)\n",
        "        criterion = nn.NLLLoss().to(device)\n",
        "        for iter in range(self.local_ep):\n",
        "            batch_loss = []\n",
        "            for batch_idx, (images, labels) in enumerate(self.trainloader):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                model.zero_grad()   \n",
        "                log_probs = model(images)\n",
        "                loss = criterion(log_probs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                batch_loss.append(loss.item())\n",
        "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
        "\n",
        "        return model.state_dict(), sum(epoch_loss) / len(epoch_loss)\n",
        "\n",
        "def test_inference(model, test_dataset):\n",
        "\n",
        "    model.eval()\n",
        "    loss, total, correct = 0.0, 0.0, 0.0\n",
        "    criterion = nn.NLLLoss().to(device)\n",
        "    testloader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "    for _, (images, labels) in enumerate(testloader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        loss += batch_loss.item()\n",
        "        _, pred_labels = torch.max(outputs, 1)\n",
        "        pred_labels = pred_labels.view(-1)\n",
        "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "        total += len(labels)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return accuracy, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjj9mvuZje9e"
      },
      "source": [
        "N = 10\n",
        "local_bs = 64\n",
        "lr = 0.01\n",
        "local_ep = 10\n",
        "EPOCHS = 5\n",
        "noise_rates = np.linspace(0, 1, N, endpoint=False)\n",
        "split_dset = mnist_iid(trainset, N, SEED)\n",
        "user_groups = {i: 0 for i in range(1, N+1)}\n",
        "noise_idx = {i: 0 for i in range(1, N+1)}\n",
        "train_datasets = {i: 0 for i in range(1, N+1)}\n",
        "for n in range(N):\n",
        "    user_groups[n+1] = np.array(list(split_dset[n]), dtype=np.int)\n",
        "    user_train_x, user_train_y = x_train[user_groups[n+1]], y_train[user_groups[n+1]]\n",
        "    user_noisy_y, noise_idx[n+1] = noisify_MNIST(noise_rates[n], 'symmetric', user_train_x, user_train_y, seed=SEED)\n",
        "    train_datasets[n+1] = TensorDataset(torch.Tensor(user_train_x),\n",
        "                                        torch.as_tensor(user_noisy_y, dtype=torch.long))\n",
        "\n",
        "\n",
        "global_model = MLP(dim_in=784, dim_hidden=64, dim_out=10, SEED=SEED)    \n",
        "global_model.to(device)\n",
        "global_model.train()\n",
        "#print(global_model)\n",
        "global_weights = global_model.state_dict()\n",
        "powerset = list(powersettool(range(1, N+1)))\n",
        "loo_sets = list(i for i in powersettool(range(1, N+1)) if len(i)==N-1)\n",
        "submodel_dict = {}  \n",
        "submodel_dict[()] = copy.deepcopy(global_model)\n",
        "accuracy_dict = {}\n",
        "loo_array = np.zeros((EPOCHS, N))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLc8p-ZFhGy2",
        "outputId": "5c63a54f-07c5-4d8d-b229-6d4154820230"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for subset in loo_sets:\n",
        "    submodel_dict[subset] = copy.deepcopy(global_model)\n",
        "    submodel_dict[subset].to(device)\n",
        "    submodel_dict[subset].train()\n",
        "\n",
        "train_loss, train_accuracy = [], []\n",
        "val_acc_list, net_list = [], []\n",
        "print_every = 2\n",
        "\n",
        "idxs_users = np.arange(1, N+1)\n",
        "total_data = sum(len(user_groups[i]) for i in range(1, N+1))\n",
        "fraction = [len(user_groups[i])/total_data for i in range(1, N+1)]\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    local_weights, local_losses = [], []\n",
        "    print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
        "    global_model.train()\n",
        "    for idx in idxs_users:\n",
        "        trainloader = DataLoader(train_datasets[idx], batch_size=local_bs, shuffle=True, worker_init_fn=seed_worker)\n",
        "        local_model = LocalUpdate(lr, local_ep, trainloader)\n",
        "        w, loss = local_model.update_weights(model=copy.deepcopy(global_model))\n",
        "        local_weights.append(copy.deepcopy(w))\n",
        "        local_losses.append(copy.deepcopy(loss))\n",
        "        \n",
        "    global_weights = average_weights(local_weights, fraction) # global_new\n",
        "    loss_avg = sum(local_losses) / len(local_losses)\n",
        "    train_loss.append(loss_avg)\n",
        "\n",
        "    gradients = calculate_gradients(local_weights, global_model.state_dict())\n",
        "\n",
        "    for subset in loo_sets: \n",
        "        subset_gradient = average_weights([gradients[i-1] for i in subset], [fraction[i-1] for i in subset])\n",
        "        subset_weights = update_weights_from_gradients(subset_gradient, submodel_dict[subset].state_dict())\n",
        "        submodel_dict[subset].load_state_dict(subset_weights)\n",
        "\n",
        "    global_model.load_state_dict(global_weights)\n",
        "    global_model.eval()\n",
        "\n",
        "    if (epoch+1) % print_every == 0:\n",
        "        print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
        "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
        "        # print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
        "\n",
        "    accuracy_dict[powerset[-1]] = test_inference(global_model, test_dataset)[0]\n",
        "\n",
        "        # Test inference for the sub-models in submodel_dict\n",
        "    for subset in loo_sets: \n",
        "        test_acc, test_loss = test_inference(submodel_dict[subset], test_dataset)\n",
        "        print(f' \\n Results after {epoch} global rounds of training:')\n",
        "        print(\"|---- Test Accuracy for {}: {:.2f}%\".format(subset, 100*test_acc))\n",
        "        accuracy_dict[subset] = test_acc\n",
        "        for i in idxs_users:\n",
        "            if i not in subset:\n",
        "                print(i, subset)\n",
        "                loo_array[epoch, i-1] = accuracy_dict[powerset[-1]] - test_acc\n",
        "\n",
        "test_acc, test_loss = test_inference(global_model, test_dataset)\n",
        "print(f' \\n Results after {EPOCHS} global rounds of training:')\n",
        "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
        "\n",
        "accuracy_dict[powerset[-1]] = test_acc\n",
        "\n",
        "trainTime = time.time() - start_time\n",
        "print('\\n Total Time: {0:0.4f}'.format(trainTime))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " | Global Training Round : 1 |\n",
            "\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 9): 86.43%\n",
            "10 (1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 10): 86.38%\n",
            "9 (1, 2, 3, 4, 5, 6, 7, 8, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 9, 10): 86.54%\n",
            "8 (1, 2, 3, 4, 5, 6, 7, 9, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 8, 9, 10): 86.51%\n",
            "7 (1, 2, 3, 4, 5, 6, 8, 9, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 7, 8, 9, 10): 86.36%\n",
            "6 (1, 2, 3, 4, 5, 7, 8, 9, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 6, 7, 8, 9, 10): 86.34%\n",
            "5 (1, 2, 3, 4, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 5, 6, 7, 8, 9, 10): 86.15%\n",
            "4 (1, 2, 3, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 4, 5, 6, 7, 8, 9, 10): 86.13%\n",
            "3 (1, 2, 4, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (1, 3, 4, 5, 6, 7, 8, 9, 10): 85.88%\n",
            "2 (1, 3, 4, 5, 6, 7, 8, 9, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:22<01:29, 22.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Results after 0 global rounds of training:\n",
            "|---- Test Accuracy for (2, 3, 4, 5, 6, 7, 8, 9, 10): 85.63%\n",
            "1 (2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
            "\n",
            " | Global Training Round : 2 |\n",
            "\n",
            " \n",
            "Avg Training Stats after 2 global rounds:\n",
            "Training Loss : 1.8486346465888175\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 9): 88.77%\n",
            "10 (1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 10): 88.90%\n",
            "9 (1, 2, 3, 4, 5, 6, 7, 8, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 9, 10): 89.05%\n",
            "8 (1, 2, 3, 4, 5, 6, 7, 9, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 8, 9, 10): 88.98%\n",
            "7 (1, 2, 3, 4, 5, 6, 8, 9, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 7, 8, 9, 10): 89.06%\n",
            "6 (1, 2, 3, 4, 5, 7, 8, 9, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 6, 7, 8, 9, 10): 88.96%\n",
            "5 (1, 2, 3, 4, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 5, 6, 7, 8, 9, 10): 88.81%\n",
            "4 (1, 2, 3, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 4, 5, 6, 7, 8, 9, 10): 88.84%\n",
            "3 (1, 2, 4, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (1, 3, 4, 5, 6, 7, 8, 9, 10): 88.65%\n",
            "2 (1, 3, 4, 5, 6, 7, 8, 9, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [00:45<01:07, 22.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Results after 1 global rounds of training:\n",
            "|---- Test Accuracy for (2, 3, 4, 5, 6, 7, 8, 9, 10): 88.23%\n",
            "1 (2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
            "\n",
            " | Global Training Round : 3 |\n",
            "\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 9): 90.07%\n",
            "10 (1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 10): 90.17%\n",
            "9 (1, 2, 3, 4, 5, 6, 7, 8, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 9, 10): 90.30%\n",
            "8 (1, 2, 3, 4, 5, 6, 7, 9, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 8, 9, 10): 90.17%\n",
            "7 (1, 2, 3, 4, 5, 6, 8, 9, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 7, 8, 9, 10): 90.26%\n",
            "6 (1, 2, 3, 4, 5, 7, 8, 9, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 6, 7, 8, 9, 10): 89.95%\n",
            "5 (1, 2, 3, 4, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 5, 6, 7, 8, 9, 10): 89.74%\n",
            "4 (1, 2, 3, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 4, 5, 6, 7, 8, 9, 10): 89.91%\n",
            "3 (1, 2, 4, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (1, 3, 4, 5, 6, 7, 8, 9, 10): 89.59%\n",
            "2 (1, 3, 4, 5, 6, 7, 8, 9, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [01:07<00:45, 22.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Results after 2 global rounds of training:\n",
            "|---- Test Accuracy for (2, 3, 4, 5, 6, 7, 8, 9, 10): 89.02%\n",
            "1 (2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
            "\n",
            " | Global Training Round : 4 |\n",
            "\n",
            " \n",
            "Avg Training Stats after 4 global rounds:\n",
            "Training Loss : 1.7884988677668128\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 9): 90.58%\n",
            "10 (1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 10): 90.76%\n",
            "9 (1, 2, 3, 4, 5, 6, 7, 8, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 9, 10): 90.78%\n",
            "8 (1, 2, 3, 4, 5, 6, 7, 9, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 8, 9, 10): 90.73%\n",
            "7 (1, 2, 3, 4, 5, 6, 8, 9, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 7, 8, 9, 10): 90.76%\n",
            "6 (1, 2, 3, 4, 5, 7, 8, 9, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 6, 7, 8, 9, 10): 90.57%\n",
            "5 (1, 2, 3, 4, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 5, 6, 7, 8, 9, 10): 90.24%\n",
            "4 (1, 2, 3, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 4, 5, 6, 7, 8, 9, 10): 90.38%\n",
            "3 (1, 2, 4, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (1, 3, 4, 5, 6, 7, 8, 9, 10): 90.11%\n",
            "2 (1, 3, 4, 5, 6, 7, 8, 9, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [01:30<00:22, 22.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Results after 3 global rounds of training:\n",
            "|---- Test Accuracy for (2, 3, 4, 5, 6, 7, 8, 9, 10): 88.71%\n",
            "1 (2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
            "\n",
            " | Global Training Round : 5 |\n",
            "\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 9): 90.92%\n",
            "10 (1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 8, 10): 91.13%\n",
            "9 (1, 2, 3, 4, 5, 6, 7, 8, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 7, 9, 10): 91.13%\n",
            "8 (1, 2, 3, 4, 5, 6, 7, 9, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 6, 8, 9, 10): 90.97%\n",
            "7 (1, 2, 3, 4, 5, 6, 8, 9, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 5, 7, 8, 9, 10): 91.22%\n",
            "6 (1, 2, 3, 4, 5, 7, 8, 9, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 4, 6, 7, 8, 9, 10): 90.77%\n",
            "5 (1, 2, 3, 4, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 3, 5, 6, 7, 8, 9, 10): 90.69%\n",
            "4 (1, 2, 3, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 2, 4, 5, 6, 7, 8, 9, 10): 90.76%\n",
            "3 (1, 2, 4, 5, 6, 7, 8, 9, 10)\n",
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (1, 3, 4, 5, 6, 7, 8, 9, 10): 90.27%\n",
            "2 (1, 3, 4, 5, 6, 7, 8, 9, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [01:52<00:00, 22.55s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Results after 4 global rounds of training:\n",
            "|---- Test Accuracy for (2, 3, 4, 5, 6, 7, 8, 9, 10): 87.40%\n",
            "1 (2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            " Results after 5 global rounds of training:\n",
            "|---- Test Accuracy: 91.14%\n",
            "\n",
            " Total Time: 113.5007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHeqBcAI9ilP",
        "outputId": "bf0b808f-d5ef-46f8-f20f-e5913aed5a21"
      },
      "source": [
        "loo_array"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6.20e-03,  3.70e-03,  1.20e-03,  1.00e-03, -9.00e-04, -1.10e-03,\n",
              "        -2.60e-03, -2.90e-03, -1.30e-03, -1.80e-03],\n",
              "       [ 6.10e-03,  1.90e-03,  0.00e+00,  3.00e-04, -1.20e-03, -2.20e-03,\n",
              "        -1.40e-03, -2.10e-03, -6.00e-04,  7.00e-04],\n",
              "       [ 1.02e-02,  4.50e-03,  1.30e-03,  3.00e-03,  9.00e-04, -2.20e-03,\n",
              "        -1.30e-03, -2.60e-03, -1.30e-03, -3.00e-04],\n",
              "       [ 1.96e-02,  5.60e-03,  2.90e-03,  4.30e-03,  1.00e-03, -9.00e-04,\n",
              "        -6.00e-04, -1.10e-03, -9.00e-04,  9.00e-04],\n",
              "       [ 3.74e-02,  8.70e-03,  3.80e-03,  4.50e-03,  3.70e-03, -8.00e-04,\n",
              "         1.70e-03,  1.00e-04,  1.00e-04,  2.20e-03]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXA7zjs95Mev",
        "outputId": "c187d6ec-c46a-48b2-eacb-4320ff7328f6"
      },
      "source": [
        "print(loo_array.sum(0))# const_loo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.0795  0.0244  0.0092  0.0131  0.0035 -0.0072 -0.0042 -0.0086 -0.004\n",
            "  0.0017]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOgMURbf5-Vq",
        "outputId": "887ca862-8f7d-4e63-fa40-00b952367574"
      },
      "source": [
        "\n",
        "base = np.arange(1, EPOCHS+1)\n",
        "\n",
        "(np.tile(base,(N,1)).T*loo_array).sum(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.3144,  0.0869,  0.0357,  0.0503,  0.0219, -0.0197, -0.0032,\n",
              "       -0.0188, -0.0095,  0.0133])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEL05Y2wn8Lz",
        "outputId": "006faba7-0afb-47a9-dc55-f5dd99398b74"
      },
      "source": [
        "np.heaviside(loo_array, 1).mean(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1. , 1. , 1. , 1. , 0.6, 0. , 0.2, 0.2, 0.2, 0.6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}