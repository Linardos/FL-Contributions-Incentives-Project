{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vs-152/FL-Contributions-Incentives-Project/blob/main/ISO_CIFAR10_OR_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgwtkMkH37Cq",
    "outputId": "d868ac63-cbe6-4e54-92da-fc1fdab26698"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pulp\n",
    "import copy\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torchvision\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from itertools import chain, combinations\n",
    "from tqdm import tqdm\n",
    "from scipy.special import comb\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import glob, nibabel as nib, pandas as pd\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import (\n",
    "    LoadImaged, EnsureChannelFirstd, Orientationd, ScaleIntensityd,\n",
    "    RandFlipd, RandSpatialCropd, Compose, SelectItemsd\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a5hHwk9S3-zy"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Xg-nE1b3w-f",
    "outputId": "0490eeb3-e60d-4360-de3b-f29126c19f3f"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 0.  Paths & meta-data\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# BRATS_DIR   = \"/users/adcw447/archive/FeTS2022/MICCAI_FeTS2022_TrainingData\"\n",
    "# VAL_DIR     = \"/users/adcw447/archive/FeTS2022/MICCAI_FeTS2022_ValidationData\"\n",
    "BRATS_DIR   = \"/mnt/c/Datasets/MICCAI_FeTS2022_TrainingData\"\n",
    "VAL_DIR     = \"/mnt/c/Datasets/MICCAI_FeTS2022_ValidationData\"\n",
    "CSV_PATH    = f\"{BRATS_DIR}/partitioning_1.csv\"     # pick 1, 2 … or sanity\n",
    "MODALITIES  = [\"flair\", \"t1\", \"t1ce\", \"t2\"]\n",
    "LABEL_KEY   = \"seg\"  # BraTS tumour mask filename ending\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1.  Read partition file → mapping   {client_id: [subjIDs]}\n",
    "# -----------------------------------------------------------\n",
    "part_df          = pd.read_csv(CSV_PATH)\n",
    "partition_map    = (\n",
    "    part_df.groupby(\"Partition_ID\")[\"Subject_ID\"]\n",
    "           .apply(list)\n",
    "           .to_dict()\n",
    ")\n",
    "NUM_CLIENTS = len(partition_map)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2.  Build a list of dicts – one per subject\n",
    "# -----------------------------------------------------------\n",
    "def build_records(subject_ids):\n",
    "    recs = []\n",
    "    for sid in subject_ids:\n",
    "        subj_dir = f\"{BRATS_DIR}/{sid}\"\n",
    "        rec = {m: f\"{subj_dir}/{sid}_{m}.nii.gz\"\n",
    "               for m in MODALITIES}\n",
    "        rec[\"seg\"] = f\"{subj_dir}/{sid}_{LABEL_KEY}.nii.gz\"\n",
    "        \n",
    "        recs.append(rec)\n",
    "    return recs\n",
    "\n",
    "def build_val_records(val_dir):\n",
    "    subjects = sorted(glob.glob(f\"{val_dir}/FeTS2022_*_flair.nii.gz\"))\n",
    "    recs = []\n",
    "    for flair_path in subjects:\n",
    "        sid = flair_path.split(\"/\")[-1].split(\"_flair\")[0]\n",
    "        subj_dir = f\"{val_dir}/{sid}\"\n",
    "        rec = {m: f\"{subj_dir}/{sid}_{m}.nii.gz\" for m in MODALITIES}\n",
    "        rec[\"seg\"] = f\"{subj_dir}/{sid}_{LABEL_KEY}.nii.gz\"   #  ← NEW\n",
    "        recs.append(rec)\n",
    "    return recs\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3.  MONAI transform pipelines  (fixed)\n",
    "# -----------------------------------------------------------\n",
    "IMG_KEYS   = [m for m in MODALITIES]\n",
    "ALL_KEYS   = IMG_KEYS + [LABEL_KEY]\n",
    "\n",
    "train_tf = Compose([\n",
    "    LoadImaged(keys=ALL_KEYS),\n",
    "    EnsureChannelFirstd(keys=ALL_KEYS),\n",
    "    Orientationd(keys=ALL_KEYS, axcodes=\"RAS\"),\n",
    "    ScaleIntensityd(keys=ALL_KEYS, minv=-1.0, maxv=1.0), # scale to [-1,1]. Diffusion Models do better if centered on a 0 mean\n",
    "    SelectItemsd(keys=ALL_KEYS),\n",
    "])\n",
    "\n",
    "val_tf = Compose([\n",
    "    LoadImaged(keys=ALL_KEYS),\n",
    "    EnsureChannelFirstd(keys=ALL_KEYS),\n",
    "    Orientationd(keys=ALL_KEYS, axcodes=\"RAS\"),\n",
    "    ScaleIntensityd(keys=MODALITIES, minv=-1.0, maxv=1.0),  #  ← OK: masks stay untouched\n",
    "    SelectItemsd(keys=ALL_KEYS),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [02:14<00:00,  1.14it/s]\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.30it/s]\n",
      "Loading dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:11<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capping at 5 for now\n",
      "{1: 153, 2: 1, 3: 4, 4: 14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4.  Build per-client datasets & dataloaders\n",
    "# -----------------------------------------------------------\n",
    "# CUT_OFF    = 4  # How many sites to discard.\n",
    "# train_datasets = {}     # {client_id: monai CacheDataset}\n",
    "# for cid, subj_list in partition_map.items():\n",
    "#     if cid > CUT_OFF:\n",
    "#         print(f\"capping at {cid} for now\")\n",
    "#         break\n",
    "#     records = build_records(subj_list)\n",
    "#     train_datasets[cid] = CacheDataset(data=records, transform=train_tf, cache_rate=0.2)\n",
    "\n",
    "# --- SUBSAMPLE FOR DEV TESTING\n",
    "\n",
    "import random                    # NEW ─ reproducible subsampling\n",
    "FRAC = 0.3 #.10                      # 10 % of every client’s cases\n",
    "SEED = 42                        # set to None for pure randomness\n",
    "rng = random.Random(SEED)        # independent RNG so you don’t disturb numpy\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4.  Build per-client datasets & dataloaders\n",
    "# -----------------------------------------------------------\n",
    "CUT_OFF = 4\n",
    "train_datasets = {}\n",
    "\n",
    "for cid, subj_list in partition_map.items():\n",
    "    if cid > CUT_OFF:\n",
    "        print(f\"capping at {cid} for now\")\n",
    "        break\n",
    "\n",
    "    # ── pick 10 % of this client’s subjects ─────────────────\n",
    "    k = max(1, int(len(subj_list) * FRAC))        # always keep ≥1 case\n",
    "    sample_ids = rng.sample(subj_list, k)\n",
    "\n",
    "    # build dataset from that subset\n",
    "    records = build_records(sample_ids)\n",
    "    train_datasets[cid] = CacheDataset(data=records, transform=train_tf, cache_rate=1)\n",
    "\n",
    "print({cid: len(ds) for cid, ds in train_datasets.items()})\n",
    "# e.g. {1: 25, 2: 1, 3: 2, 4: 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 7.  Build test dataset & dataloader\n",
    "# -----------------------------------------------------------\n",
    "val_records  = build_val_records(VAL_DIR)\n",
    "test_dataset  = CacheDataset(data=val_records, transform=val_tf, cache_rate=1)\n",
    "# test_loader   = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DrrjBXZg4rEN"
   },
   "outputs": [],
   "source": [
    "# def test_inference(model, test_dataset):\n",
    "#     # --------- INFERENCE FOR SEGMENTATION\n",
    "#     model.eval()\n",
    "#     total_dice = 0.0\n",
    "#     testloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in testloader:\n",
    "#             images = torch.stack([batch[k] for k in [\"flair\", \"t1\", \"t1ce\", \"t2\"]], dim=1).squeeze(2).to(device)  # (B, 4, D, H, W)\n",
    "#             labels = batch[\"seg\"].long().to(device)  # (B, 1, D, H, W)\n",
    "\n",
    "#             outputs = model(images)\n",
    "#             preds = torch.argmax(outputs, dim=1, keepdim=True)  # (B, 1, D, H, W)\n",
    "\n",
    "#             # Simple Dice calculation (per class optional)\n",
    "#             intersection = (preds == labels).sum().item()\n",
    "#             total = labels.numel()\n",
    "#             total_dice += 2.0 * intersection / (total + preds.numel())\n",
    "\n",
    "#     return total_dice / (len(testloader) + 0.0000001)\n",
    "\n",
    "def test_inference(model, test_dataset):\n",
    "    \"\"\"\n",
    "    Computes mean Dice across the three BraTS channels\n",
    "    (whole tumour, tumour-core, enhancing-core).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float   ─ mean Dice in the range [0, 1]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    dice_sum = torch.zeros(3, device=device)   # accumulate per-class Dice\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # ---- image tensor ------------------------------------------------\n",
    "            img = torch.cat([batch[k] for k in (\"flair\", \"t1\", \"t1ce\", \"t2\")],\n",
    "                            dim=1).to(device)                     # (B,4,D,H,W)\n",
    "\n",
    "            # ---- ground-truth mask → one-hot (B,3,D,H,W) ---------------------\n",
    "            raw    = batch[\"seg\"].squeeze(1).cpu().numpy()       # (B,D,H,W)\n",
    "            target = torch.tensor(\n",
    "                        preprocess_mask_labels(raw),\n",
    "                        dtype=torch.float32, device=device\n",
    "                     )\n",
    "\n",
    "            # ---- network prediction → one-hot (B,3,D,H,W) --------------------\n",
    "            logits  = model(img)                                 # (B,3,D,H,W)\n",
    "            pred_ch = torch.argmax(logits, dim=1)                # (B,D,H,W)\n",
    "            pred    = torch.nn.functional.one_hot(\n",
    "                          pred_ch, num_classes=3\n",
    "                      ).permute(0,4,1,2,3).float()\n",
    "\n",
    "            # ---- per-class Dice ---------------------------------------------\n",
    "            intersect = 2 * (pred * target).sum(dim=(2,3,4))     # (B,3)\n",
    "            denom     = (pred + target).sum(dim=(2,3,4)) + 1e-6\n",
    "            dice_sum += (intersect / denom).squeeze(0)           # add (3,)\n",
    "            n_batches += 1\n",
    "\n",
    "    return (dice_sum / n_batches).mean().item()                  # scalar Dice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WTUtuKlu4Ddo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got 4 clients\n"
     ]
    }
   ],
   "source": [
    "# N = 10 #srch\n",
    "\n",
    "# after you build train_datasets\n",
    "idxs_users = np.array(sorted(train_datasets.keys()))\n",
    "N = len(idxs_users)\n",
    "\n",
    "# N = list(train_datasets.keys())[-1]\n",
    "print(f\"We got {N} clients\")\n",
    "local_bs = 2\n",
    "lr = 1e-4\n",
    "local_ep = 1\n",
    "EPOCHS = 1\n",
    "\n",
    "# noise_rates = np.linspace(0, 1, N, endpoint=False)\n",
    "# split_dset = mnist_iid(trainset, N)\n",
    "# user_groups = {i: 0 for i in range(1, N+1)}\n",
    "# noise_idx = {i: 0 for i in range(1, N+1)}\n",
    "# train_datasets = {i: 0 for i in range(1, N+1)}\n",
    "# for n in range(N):\n",
    "#     user_groups[n+1] = np.array(list(split_dset[n]), dtype=np.int)\n",
    "#     user_train_x, user_train_y = x_train[user_groups[n+1]], y_train[user_groups[n+1]]\n",
    "#     user_noisy_y, noise_idx[n+1] = noisify_MNIST(noise_rates[n], 'symmetric', user_train_x, user_train_y)\n",
    "    \n",
    "#     train_datasets[n+1] = CustomTensorDataset((user_train_x, user_noisy_y), transform_train)\n",
    "\n",
    "def copy_batchnorm_stats(subset_weights, global_model_state_dict):\n",
    "    for pair_1, pair_2 in zip(subset_weights.items(), global_model_state_dict.items()):\n",
    "        if ('running' in pair_1[0]) or ('batches' in pair_1[0]):\n",
    "            subset_weights[pair_1[0]] = global_model_state_dict[pair_1[0]]\n",
    "    \n",
    "    return subset_weights\n",
    "\n",
    "\n",
    "global_model = ResUNet3D(in_channels=4, out_channels=3).to(device) # Segmentation model used in FeTS\n",
    "\n",
    "global_model.to(device)\n",
    "global_model.train()\n",
    "\n",
    "global_weights = global_model.state_dict()\n",
    "powerset = list(powersettool(range(1, N+1)))[1:]          # discard ()\n",
    "# … the rest unchanged …\n",
    "\n",
    "\n",
    "# submodel_dict = {}\n",
    "# submodel_dict[()] = copy.deepcopy(global_model)\n",
    "# Change instead to storing the submodel weights to disk using torch.save\n",
    "import os\n",
    "submodel_dir = \"submodels\"\n",
    "os.makedirs(submodel_dir, exist_ok=True)\n",
    "submodel_file_template = os.path.join(submodel_dir, \"submodel_{}.pth\")\n",
    "global_model_path = os.path.join(submodel_dir, \"global_model.pth\")\n",
    "# Save the global model weights\n",
    "torch.save(global_weights, global_model_path)\n",
    "\n",
    "accuracy_dict = {}\n",
    "# accuracy_dict[()] = 0.0                                # utility of empty coalition\n",
    "shapley_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,),\n",
       " (2,),\n",
       " (3,),\n",
       " (4,),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (3, 4),\n",
       " (1, 2, 3),\n",
       " (1, 2, 4),\n",
       " (1, 3, 4),\n",
       " (2, 3, 4),\n",
       " (1, 2, 3, 4)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvXdFSu24Iq6",
    "outputId": "76c751e1-1bbf-49a6-9803-df0517c62df5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 1 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                            | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|████████████████████████████████▊                                                                                                  | 1/4 [01:58<05:55, 118.54s/it]\u001b[A\n",
      " 50%|██████████████████████████████████████████████████████████████████                                                                  | 2/4 [02:00<01:39, 49.83s/it]\u001b[A\n",
      " 75%|███████████████████████████████████████████████████████████████████████████████████████████████████                                 | 3/4 [02:04<00:28, 28.96s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [02:16<00:00, 34.02s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [02:16<00:00, 136.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "Training Loss : 5.658808510121587\n",
      " \n",
      " Results after 1 global rounds of training:\n",
      "|---- Test DSC: nan%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Store initial single user submodels as the initial global model\n",
    "for user in range(1, N+1):\n",
    "    model_path = submodel_file_template.format(user)\n",
    "    # global_weights is already global_model.state_dict()  \n",
    "    torch.save(global_weights, model_path)\n",
    " \n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "print_every = 1\n",
    "\n",
    "idxs_users = np.arange(1, N+1)\n",
    "# total_data = sum(len(user_groups[i]) for i in range(1, N+1))\n",
    "# fraction = [len(user_groups[i])/total_data for i in range(1, N+1)]\n",
    "\n",
    "# ── collect dataset sizes ──────────────────────────────────────────────────\n",
    "# MONAI's CacheDataset inherits __len__, so `len(ds)` is cheap:\n",
    "sizes = {k: len(ds) for k, ds in train_datasets.items()}\n",
    "\n",
    "\n",
    "# ── total samples across all clients ───────────────────────────────────────\n",
    "total_data = sum(sizes.values())\n",
    "\n",
    "# ── FedAvg weight (a.k.a. fraction) for each client ────────────────────────\n",
    "# Keep the list in key order 1…N so it lines up with your loops later.\n",
    "fraction = [sizes[i] / total_data for i in range(1, N + 1)]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
    "    global_model.train()\n",
    "    for idx in tqdm(idxs_users):\n",
    "        trainloader = DataLoader(train_datasets[idx], batch_size=local_bs, shuffle=True,\n",
    "                                 num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "        local_trainer = LocalUpdateMONAI(\n",
    "            lr=lr,\n",
    "            local_ep=local_ep,\n",
    "            trainloader=trainloader,\n",
    "            img_keys=(\"flair\",\"t1\",\"t1ce\",\"t2\"),\n",
    "            label_key=\"seg\",\n",
    "        )\n",
    "        \n",
    "        local_model = copy.deepcopy(global_model)  # Ensure local_model is a fresh copy\n",
    "        w, loss = local_trainer.update_weights(model=local_model)\n",
    "        local_weights.append(w)\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "    del local_model  # Free memory after each client update\n",
    "    global_weights = average_weights(local_weights, fraction) # This operates on a list of state_dicts\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # calculate weight updates aka pseudo-gradients Delta_i = local_weight - old_global_weight\n",
    "    gradients = calculate_gradients(local_weights, global_model.state_dict())\n",
    "\n",
    "    # update the single user submodels stored on disk based on the pseudo-gradients\n",
    "    for i in range(1, N+1):\n",
    "        user_path = submodel_file_template.format(i)\n",
    "        prev_user_weights = torch.load(user_path)  # Load previous user weights\n",
    "        user_weights = update_weights_from_gradients(gradients[i-1], prev_user_weights)\n",
    "        \n",
    "        # unsure the following line is needed in this context\n",
    "        # subset_weights = copy_batchnorm_stats(subset_weights, global_model.state_dict())\n",
    "        \n",
    "        # Save the updated user weights back to disk\n",
    "        torch.save(user_weights, user_path)\n",
    "\n",
    "    # update the global model with the averaged weights\n",
    "    global_model.load_state_dict(global_weights)\n",
    "    global_model.eval()\n",
    "\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "        # print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
    "\n",
    "test_dsc = test_inference(global_model, test_dataset)\n",
    "print(f' \\n Results after {EPOCHS} global rounds of training:')\n",
    "print(\"|---- Test DSC: {:.2f}%\".format(100*test_dsc))\n",
    "\n",
    "accuracy_dict[powerset[-1]] = test_dsc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (1,): nan%\n",
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (2,): nan%\n",
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (3,): nan%\n",
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (4,): nan%\n",
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (1, 2): nan%\n",
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (1, 3): nan%\n",
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (1, 4): nan%\n",
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (2, 3): nan%\n",
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (2, 4): nan%\n",
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (3, 4): nan%\n",
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (1, 2, 3): nan%\n",
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (1, 2, 4): nan%\n",
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (1, 3, 4): nan%\n",
      " \n",
      " Results after 1 global rounds of training (for OR): \n",
      "|---- Test DSC for (2, 3, 4): nan%\n",
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/aklina/miniconda3/envs/m_quant/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/6f3fd1f48c7e4b4fbf669a65855eb4bf-pulp.mps -timeMode elapsed -branch -printingOptions all -solution /tmp/6f3fd1f48c7e4b4fbf669a65855eb4bf-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 22 COLUMNS\n",
      "At line 76 RHS\n",
      "Bad image at line 77 <     RHS       C0000000   nan >\n",
      "Bad image at line 79 <     RHS       C0000002   nan >\n",
      "Bad image at line 80 <     RHS       C0000003   nan >\n",
      "Bad image at line 81 <     RHS       C0000004   nan >\n",
      "Bad image at line 82 <     RHS       C0000005   nan >\n",
      "Bad image at line 83 <     RHS       C0000006   nan >\n",
      "Bad image at line 84 <     RHS       C0000007   nan >\n",
      "Bad image at line 85 <     RHS       C0000008   nan >\n",
      "Bad image at line 86 <     RHS       C0000009   nan >\n",
      "Bad image at line 87 <     RHS       C0000010   nan >\n",
      "Bad image at line 88 <     RHS       C0000011   nan >\n",
      "Bad image at line 89 <     RHS       C0000012   nan >\n",
      "Bad image at line 90 <     RHS       C0000013   nan >\n",
      "Bad image at line 91 <     RHS       C0000014   nan >\n",
      "Bad image at line 92 <     RHS       C0000015   nan >\n",
      "Bad image at line 93 <     RHS       C0000016   nan >\n",
      "At line 94 BOUNDS\n",
      "At line 96 ENDATA\n",
      "Problem MODEL has 17 rows, 5 columns and 52 elements\n",
      "Coin0008I MODEL read with 16 errors\n",
      "There were 16 errors on input\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "** Current model not valid\n",
      "Option for printingOptions changed from normal to all\n",
      "** Current model not valid\n",
      "No match for /tmp/6f3fd1f48c7e4b4fbf669a65855eb4bf-pulp.sol - ? for list of commands\n",
      "Total time (CPU seconds):       0.00   (Wallclock seconds):       0.00\n",
      "\n"
     ]
    },
    {
     "ename": "PulpSolverError",
     "evalue": "Pulp: Error while executing /home/aklina/miniconda3/envs/m_quant/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPulpSolverError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m shapTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     24\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 25\u001b[0m lc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mleast_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccuracy_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m LCTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     27\u001b[0m totalShapTime \u001b[38;5;241m=\u001b[39m trainTime \u001b[38;5;241m+\u001b[39m shapTime\n",
      "File \u001b[0;32m~/FL-Contributions-Incentives-Project/utils.py:134\u001b[0m, in \u001b[0;36mleast_core\u001b[0;34m(char_function_dict, N)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m char_function_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    133\u001b[0m     model \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pulp\u001b[38;5;241m.\u001b[39mlpSum(x[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m key) \u001b[38;5;241m+\u001b[39m e \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 134\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m print_solution(model)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/m_quant/lib/python3.10/site-packages/pulp/pulp.py:2058\u001b[0m, in \u001b[0;36mLpProblem.solve\u001b[0;34m(self, solver, **kwargs)\u001b[0m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;66;03m# time it\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstartClock()\n\u001b[0;32m-> 2058\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactualSolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2059\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopClock()\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestoreObjective(wasNone, dummyVar)\n",
      "File \u001b[0;32m~/miniconda3/envs/m_quant/lib/python3.10/site-packages/pulp/apis/coin_api.py:140\u001b[0m, in \u001b[0;36mCOIN_CMD.actualSolve\u001b[0;34m(self, lp, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mactualSolve\u001b[39m(\u001b[38;5;28mself\u001b[39m, lp: LpProblem, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Solve a well formulated lp problem\"\"\"\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve_CBC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/m_quant/lib/python3.10/site-packages/pulp/apis/coin_api.py:231\u001b[0m, in \u001b[0;36mCOIN_CMD.solve_CBC\u001b[0;34m(self, lp, use_mps)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(tmpSol):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PulpSolverError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPulp: Error while executing \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m    232\u001b[0m (\n\u001b[1;32m    233\u001b[0m     status,\n\u001b[1;32m    234\u001b[0m     values,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     sol_status,\n\u001b[1;32m    239\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadsol_MPS(tmpSol, lp, vs, variablesNames, constraintsNames)\n\u001b[1;32m    240\u001b[0m lp\u001b[38;5;241m.\u001b[39massignVarsVals(values)\n",
      "\u001b[0;31mPulpSolverError\u001b[0m: Pulp: Error while executing /home/aklina/miniconda3/envs/m_quant/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc"
     ]
    }
   ],
   "source": [
    "\n",
    "# ADJUSTED-OR APPROX\n",
    "for subset in powerset[:-1]: \n",
    "    if len(subset) > 1:\n",
    "        # calculate the average of the subset of weights from list of all the weights in the subset\n",
    "        subset_weights = average_weights([torch.load(submodel_file_template.format(user)) for user in subset], [fraction[i-1] for i in subset])\n",
    "    elif len(subset) == 1:\n",
    "        # for single user submodels, just load the weights from disk, no need to average\n",
    "        subset_weights = torch.load(submodel_file_template.format(subset[0]))\n",
    "        # need to make a model from the averaged weights to test it\n",
    "    else:\n",
    "        continue\n",
    "    submodel = ResUNet3D(in_channels=4, out_channels=3).to(device)\n",
    "    submodel.load_state_dict(subset_weights)   \n",
    "    test_dsc = test_inference(submodel,test_dataset)\n",
    "    print(f' \\n Results after {EPOCHS} global rounds of training (for OR): ')\n",
    "    print(\"|---- Test DSC for {}: {:.2f}%\".format(subset, 100*test_dsc))\n",
    "    accuracy_dict[subset] = test_dsc\n",
    "    del submodel  # Free memory after each subset test\n",
    "\n",
    "trainTime = time.time() - start_time\n",
    "start_time = time.time()\n",
    "shapley_dict = shapley(accuracy_dict, N)\n",
    "shapTime = time.time() - start_time\n",
    "start_time = time.time()\n",
    "lc_dict = least_core(accuracy_dict, N)\n",
    "LCTime = time.time() - start_time\n",
    "totalShapTime = trainTime + shapTime\n",
    "totalLCTime = trainTime + LCTime\n",
    "print(f'\\n ACCURACY: {accuracy_dict[powerset[-1]]}')\n",
    "print('\\n Total Time Shapley: {0:0.4f}'.format(totalShapTime))\n",
    "print('\\n Total Time LC: {0:0.4f}'.format(totalLCTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapley allocation:\")\n",
    "for cid, phi in shapley_dict.items():\n",
    "    print(f\" client {cid}: {phi:.4f}\")\n",
    "\n",
    "print(\"\\nLeast-Core allocation:\")\n",
    "for var in lc_dict.variables():\n",
    "    if var.name.startswith(\"x(\"):\n",
    "        print(f\" client {var.name[2:-1]}: {var.value():.4f}\")\n",
    "print(f\" e (slack): {lc_dict.variablesDict()['e'].value():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOBqzOdQRfIe"
   },
   "outputs": [],
   "source": [
    "def stats(vector):\n",
    "    n = len(vector)\n",
    "    egal = np.array([1/n for i in range(n)])\n",
    "    normalised = np.array(vector / vector.sum())\n",
    "    msg = f'Original vector: {vector}\\n'\n",
    "    msg += f'Normalised vector: {normalised}\\n'\n",
    "    msg += f'Max Dif: {normalised.max()-normalised.min()}\\n'\n",
    "    msg += f'Distance: {np.linalg.norm(normalised-egal)}\\n'\n",
    "\n",
    "    msg += f'Budget: {vector.sum()}\\n'\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWyxrrCUwOxO",
    "outputId": "50b2f298-6572-4de7-8942-4065a3e2b0c8"
   },
   "outputs": [],
   "source": [
    "stats(np.array(list(shapley_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZ3ZJVjFJA1c",
    "outputId": "86f4a6b1-a728-45ea-83d6-7d9913b59823"
   },
   "outputs": [],
   "source": [
    "stats(np.array([i.value() for i in lc_dict.variables()])[1:])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMNKnjfLFi/+UJW/ZI4jUCD",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ISO CIFAR10 OR FINAL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
