{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vs-152/FL-Contributions-Incentives-Project/blob/main/ISO_CIFAR10_OR_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgwtkMkH37Cq",
    "outputId": "d868ac63-cbe6-4e54-92da-fc1fdab26698"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pulp\n",
    "import copy\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torchvision\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from itertools import chain, combinations\n",
    "from tqdm import tqdm\n",
    "from scipy.special import comb\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import glob, nibabel as nib, pandas as pd\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import (\n",
    "    LoadImaged, EnsureChannelFirstd, Orientationd, ScaleIntensityd,\n",
    "    RandFlipd, RandSpatialCropd, Compose, SelectItemsd\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a5hHwk9S3-zy"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Xg-nE1b3w-f",
    "outputId": "0490eeb3-e60d-4360-de3b-f29126c19f3f"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 0.  Paths & meta-data\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "BRATS_DIR   = \"/mnt/c/Datasets/MICCAI_FeTS2022_TrainingData\"\n",
    "VAL_DIR     = \"/mnt/c/Datasets/MICCAI_FeTS2022_ValidationData\"\n",
    "CSV_PATH    = f\"{BRATS_DIR}/partitioning_1.csv\"     # pick 1, 2 … or sanity\n",
    "MODALITIES  = [\"flair\", \"t1\", \"t1ce\", \"t2\"]\n",
    "LABEL_KEY   = \"seg\"  # BraTS tumour mask filename ending\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1.  Read partition file → mapping   {client_id: [subjIDs]}\n",
    "# -----------------------------------------------------------\n",
    "part_df          = pd.read_csv(CSV_PATH)\n",
    "partition_map    = (\n",
    "    part_df.groupby(\"Partition_ID\")[\"Subject_ID\"]\n",
    "           .apply(list)\n",
    "           .to_dict()\n",
    ")\n",
    "NUM_CLIENTS = len(partition_map)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2.  Build a list of dicts – one per subject\n",
    "# -----------------------------------------------------------\n",
    "def build_records(subject_ids):\n",
    "    recs = []\n",
    "    for sid in subject_ids:\n",
    "        subj_dir = f\"{BRATS_DIR}/{sid}\"\n",
    "        rec = {m: f\"{subj_dir}/{sid}_{m}.nii.gz\"\n",
    "               for m in MODALITIES}\n",
    "        rec[\"seg\"] = f\"{subj_dir}/{sid}_{LABEL_KEY}.nii.gz\"\n",
    "        recs.append(rec)\n",
    "    return recs\n",
    "\n",
    "def build_val_records(val_dir):\n",
    "    subjects = sorted(glob.glob(f\"{val_dir}/FeTS2022_*_flair.nii.gz\"))\n",
    "    recs = []\n",
    "    for flair_path in subjects:\n",
    "        sid = flair_path.split(\"/\")[-1].split(\"_flair\")[0]\n",
    "        subj_dir = f\"{val_dir}/{sid}\"\n",
    "        rec = {m: f\"{subj_dir}/{sid}_{m}.nii.gz\" for m in MODALITIES}\n",
    "        recs.append(rec)\n",
    "    return recs\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3.  MONAI transform pipelines  (fixed)\n",
    "# -----------------------------------------------------------\n",
    "IMG_KEYS   = [m for m in MODALITIES]\n",
    "ALL_KEYS   = IMG_KEYS + [LABEL_KEY]\n",
    "\n",
    "train_tf = Compose([\n",
    "    LoadImaged(keys=ALL_KEYS),\n",
    "    EnsureChannelFirstd(keys=ALL_KEYS),\n",
    "    Orientationd(keys=ALL_KEYS, axcodes=\"RAS\"),\n",
    "    ScaleIntensityd(keys=ALL_KEYS, minv=-1.0, maxv=1.0), # scale to [-1,1]. Diffusion Models do better if centered on a 0 mean\n",
    "    SelectItemsd(keys=ALL_KEYS),\n",
    "])\n",
    "\n",
    "val_tf = Compose([\n",
    "    LoadImaged(keys=MODALITIES),\n",
    "    EnsureChannelFirstd(keys=MODALITIES),\n",
    "    Orientationd(keys=MODALITIES, axcodes=\"RAS\"),\n",
    "    ScaleIntensityd(keys=MODALITIES, minv=-1.0, maxv=1.0),\n",
    "    SelectItemsd(keys=MODALITIES),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset:  15%|█████████▋                                                       | 76/511 [01:00<06:09,  1.18it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4.  Build per-client datasets & dataloaders\n",
    "# -----------------------------------------------------------\n",
    "# CUT_OFF    = 4  # How many sites to discard.\n",
    "# train_datasets = {}     # {client_id: monai CacheDataset}\n",
    "# for cid, subj_list in partition_map.items():\n",
    "#     if cid > CUT_OFF:\n",
    "#         print(f\"capping at {cid} for now\")\n",
    "#         break\n",
    "#     records = build_records(subj_list)\n",
    "#     train_datasets[cid] = CacheDataset(data=records, transform=train_tf, cache_rate=0.2)\n",
    "\n",
    "# --- SUBSAMPLE FOR DEV TESTING\n",
    "\n",
    "import random                    # NEW ─ reproducible subsampling\n",
    "FRAC = 1 #.10                      # 10 % of every client’s cases\n",
    "SEED = 42                        # set to None for pure randomness\n",
    "rng = random.Random(SEED)        # independent RNG so you don’t disturb numpy\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4.  Build per-client datasets & dataloaders\n",
    "# -----------------------------------------------------------\n",
    "CUT_OFF = 4\n",
    "train_datasets = {}\n",
    "\n",
    "for cid, subj_list in partition_map.items():\n",
    "    if cid > CUT_OFF:\n",
    "        print(f\"capping at {cid} for now\")\n",
    "        break\n",
    "\n",
    "    # ── pick 10 % of this client’s subjects ─────────────────\n",
    "    k = max(1, int(len(subj_list) * FRAC))        # always keep ≥1 case\n",
    "    sample_ids = rng.sample(subj_list, k)\n",
    "\n",
    "    # build dataset from that subset\n",
    "    records = build_records(sample_ids)\n",
    "    train_datasets[cid] = CacheDataset(data=records, transform=train_tf, cache_rate=1)\n",
    "\n",
    "print({cid: len(ds) for cid, ds in train_datasets.items()})\n",
    "# e.g. {1: 25, 2: 1, 3: 2, 4: 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 7.  Build test dataset & dataloader\n",
    "# -----------------------------------------------------------\n",
    "val_records  = build_val_records(VAL_DIR)\n",
    "test_dataset  = CacheDataset(data=val_records, transform=val_tf, cache_rate=1)\n",
    "# test_loader   = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrrjBXZg4rEN"
   },
   "outputs": [],
   "source": [
    "def test_inference(model, test_dataset):\n",
    "    # --------- INFERENCE FOR SEGMENTATION\n",
    "    model.eval()\n",
    "    total_dice = 0.0\n",
    "    testloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images = torch.stack([batch[k] for k in [\"flair\", \"t1\", \"t1ce\", \"t2\"]], dim=1).squeeze(2).to(device)  # (B, 4, D, H, W)\n",
    "            labels = batch[\"seg\"].long().to(device)  # (B, 1, D, H, W)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1, keepdim=True)  # (B, 1, D, H, W)\n",
    "\n",
    "            # Simple Dice calculation (per class optional)\n",
    "            intersection = (preds == labels).sum().item()\n",
    "            total = labels.numel()\n",
    "            total_dice += 2.0 * intersection / (total + preds.numel())\n",
    "\n",
    "    return total_dice / len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTUtuKlu4Ddo"
   },
   "outputs": [],
   "source": [
    "# N = 10 #srch\n",
    "\n",
    "# after you build train_datasets\n",
    "idxs_users = np.array(sorted(train_datasets.keys()))\n",
    "N = len(idxs_users)\n",
    "\n",
    "# N = list(train_datasets.keys())[-1]\n",
    "print(f\"We got {N} clients\")\n",
    "local_bs = 4\n",
    "lr = 1e-4\n",
    "local_ep = 1\n",
    "EPOCHS = 1\n",
    "\n",
    "# noise_rates = np.linspace(0, 1, N, endpoint=False)\n",
    "# split_dset = mnist_iid(trainset, N)\n",
    "# user_groups = {i: 0 for i in range(1, N+1)}\n",
    "# noise_idx = {i: 0 for i in range(1, N+1)}\n",
    "# train_datasets = {i: 0 for i in range(1, N+1)}\n",
    "# for n in range(N):\n",
    "#     user_groups[n+1] = np.array(list(split_dset[n]), dtype=np.int)\n",
    "#     user_train_x, user_train_y = x_train[user_groups[n+1]], y_train[user_groups[n+1]]\n",
    "#     user_noisy_y, noise_idx[n+1] = noisify_MNIST(noise_rates[n], 'symmetric', user_train_x, user_train_y)\n",
    "    \n",
    "#     train_datasets[n+1] = CustomTensorDataset((user_train_x, user_noisy_y), transform_train)\n",
    "\n",
    "def copy_batchnorm_stats(subset_weights, global_model_state_dict):\n",
    "    for pair_1, pair_2 in zip(subset_weights.items(), global_model_state_dict.items()):\n",
    "        if ('running' in pair_1[0]) or ('batches' in pair_1[0]):\n",
    "            subset_weights[pair_1[0]] = global_model_state_dict[pair_1[0]]\n",
    "    \n",
    "    return subset_weights\n",
    "\n",
    "\n",
    "global_model = ResUNet3D(in_channels=4, out_channels=3).to(device) # Segmentation model used in FeTS\n",
    "\n",
    "global_model.to(device)\n",
    "global_model.train()\n",
    "\n",
    "global_weights = global_model.state_dict()\n",
    "powerset = list(powersettool(range(1, N+1)))\n",
    "\n",
    "# submodel_dict = {}\n",
    "# submodel_dict[()] = copy.deepcopy(global_model)\n",
    "# Change instead to storing the submodel weights to disk using torch.save\n",
    "import os\n",
    "submodel_dir = \"submodels\"\n",
    "os.makedirs(submodel_dir, exist_ok=True)\n",
    "submodel_file_template = os.path.join(submodel_dir, \"submodel_{}.pth\")\n",
    "global_model_path = os.path.join(submodel_dir, \"global_model.pth\")\n",
    "# Save the global model weights\n",
    "torch.save(global_weights, global_model_path)\n",
    "\n",
    "accuracy_dict = {}\n",
    "shapley_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvXdFSu24Iq6",
    "outputId": "76c751e1-1bbf-49a6-9803-df0517c62df5"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Store initial single user submodels as the initial global model\n",
    "for user in range(1, N+1):\n",
    "    model_path = submodel_file_template.format(user)\n",
    "    # global_weights is already global_model.state_dict()  \n",
    "    torch.save(global_weights, model_path)\n",
    " \n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "print_every = 1\n",
    "\n",
    "idxs_users = np.arange(1, N+1)\n",
    "# total_data = sum(len(user_groups[i]) for i in range(1, N+1))\n",
    "# fraction = [len(user_groups[i])/total_data for i in range(1, N+1)]\n",
    "\n",
    "# ── collect dataset sizes ──────────────────────────────────────────────────\n",
    "# MONAI's CacheDataset inherits __len__, so `len(ds)` is cheap:\n",
    "sizes = {k: len(ds) for k, ds in train_datasets.items()}\n",
    "\n",
    "\n",
    "# ── total samples across all clients ───────────────────────────────────────\n",
    "total_data = sum(sizes.values())\n",
    "\n",
    "# ── FedAvg weight (a.k.a. fraction) for each client ────────────────────────\n",
    "# Keep the list in key order 1…N so it lines up with your loops later.\n",
    "fraction = [sizes[i] / total_data for i in range(1, N + 1)]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
    "    global_model.train()\n",
    "    for idx in idxs_users:\n",
    "        trainloader = DataLoader(train_datasets[idx], batch_size=local_bs, shuffle=True)\n",
    "        \n",
    "        local_trainer = LocalUpdateMONAI(\n",
    "            lr=lr,\n",
    "            local_ep=local_ep,\n",
    "            trainloader=trainloader,\n",
    "            img_keys=(\"flair\",\"t1\",\"t1ce\",\"t2\"),\n",
    "            label_key=\"seg\",\n",
    "        )\n",
    "        \n",
    "        local_model = copy.deepcopy(global_model)  # Ensure local_model is a fresh copy\n",
    "        w, loss = local_trainer.update_weights(model=local_model)\n",
    "        local_weights.append(w)\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "    del local_model  # Free memory after each client update\n",
    "    global_weights = average_weights(local_weights, fraction) # This operates on a list of state_dicts\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # calculate weight updates aka pseudo-gradients Delta_i = local_weight - old_global_weight\n",
    "    gradients = calculate_gradients(local_weights, global_model.state_dict())\n",
    "\n",
    "    # update the single user submodels stored on disk based on the pseudo-gradients\n",
    "    for i in range(1, N+1):\n",
    "        user_path = submodel_file_template.format(i)\n",
    "        prev_user_weights = torch.load(user_path)  # Load previous user weights\n",
    "        user_weights = update_weights_from_gradients(gradients[i-1], prev_user_weights)\n",
    "        \n",
    "        # unsure the following line is needed in this context\n",
    "        # subset_weights = copy_batchnorm_stats(subset_weights, global_model.state_dict())\n",
    "        \n",
    "        # Save the updated user weights back to disk\n",
    "        torch.save(user_weights, user_path)\n",
    "\n",
    "    # update the global model with the averaged weights\n",
    "    global_model.load_state_dict(global_weights)\n",
    "    global_model.eval()\n",
    "\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "        # print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
    "\n",
    "test_dsc = test_inference(global_model, test_dataset)\n",
    "print(f' \\n Results after {EPOCHS} global rounds of training:')\n",
    "print(\"|---- Test DSC: {:.2f}%\".format(100*test_dsc))\n",
    "\n",
    "accuracy_dict[powerset[-1]] = test_dsc\n",
    "\n",
    "# ADJUSTED-OR APPROX\n",
    "for subset in powerset[:-1]: \n",
    "    if len(subset) > 1:\n",
    "        # calculate the average of the subset of weights from list of all the weights in the subset\n",
    "        subset_weights = average_weights([torch.load(submodel_file_template.format(user)) for user in subset], [fraction[i-1] for i in subset])\n",
    "    elif len(subset) == 1:\n",
    "        # for single user submodels, just load the weights from disk, no need to average\n",
    "        subset_weights = torch.load(submodel_file_template.format(subset[0]))\n",
    "        # need to make a model from the averaged weights to test it\n",
    "    submodel = ResUNet3D(in_channels=4, out_channels=3).to(device)\n",
    "    submodel.load_state_dict(subset_weights)   \n",
    "    test_dsc = test_inference(submodel,test_dataset)\n",
    "    print(f' \\n Results after {EPOCHS} global rounds of training (for OR): ')\n",
    "    print(\"|---- Test DSC for {}: {:.2f}%\".format(subset, 100*test_dsc))\n",
    "    accuracy_dict[subset] = test_dsc\n",
    "    del submodel  # Free memory after each subset test\n",
    "\n",
    "trainTime = time.time() - start_time\n",
    "start_time = time.time()\n",
    "shapley_dict = shapley(accuracy_dict, N)\n",
    "shapTime = time.time() - start_time\n",
    "start_time = time.time()\n",
    "lc_dict = least_core(accuracy_dict, N)\n",
    "LCTime = time.time() - start_time\n",
    "totalShapTime = trainTime + shapTime\n",
    "totalLCTime = trainTime + LCTime\n",
    "print(f'\\n ACCURACY: {accuracy_dict[powerset[-1]]}')\n",
    "print('\\n Total Time Shapley: {0:0.4f}'.format(totalShapTime))\n",
    "print('\\n Total Time LC: {0:0.4f}'.format(totalLCTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOBqzOdQRfIe"
   },
   "outputs": [],
   "source": [
    "def stats(vector):\n",
    "    n = len(vector)\n",
    "    egal = np.array([1/n for i in range(n)])\n",
    "    normalised = np.array(vector / vector.sum())\n",
    "    msg = f'Original vector: {vector}\\n'\n",
    "    msg += f'Normalised vector: {normalised}\\n'\n",
    "    msg += f'Max Dif: {normalised.max()-normalised.min()}\\n'\n",
    "    msg += f'Distance: {np.linalg.norm(normalised-egal)}\\n'\n",
    "\n",
    "    msg += f'Budget: {vector.sum()}\\n'\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWyxrrCUwOxO",
    "outputId": "50b2f298-6572-4de7-8942-4065a3e2b0c8"
   },
   "outputs": [],
   "source": [
    "stats(np.array(list(shapley_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZ3ZJVjFJA1c",
    "outputId": "86f4a6b1-a728-45ea-83d6-7d9913b59823"
   },
   "outputs": [],
   "source": [
    "stats(np.array([i.value() for i in lc_dict.variables()])[1:])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMNKnjfLFi/+UJW/ZI4jUCD",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ISO CIFAR10 OR FINAL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
