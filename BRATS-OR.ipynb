{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vs-152/FL-Contributions-Incentives-Project/blob/main/ISO_CIFAR10_OR_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgwtkMkH37Cq",
    "outputId": "d868ac63-cbe6-4e54-92da-fc1fdab26698"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pulp\n",
    "import copy\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torchvision\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from itertools import chain, combinations\n",
    "from tqdm import tqdm\n",
    "from scipy.special import comb\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import glob, nibabel as nib, pandas as pd\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import (\n",
    "    LoadImaged, EnsureChannelFirstd, Orientationd, ScaleIntensityd,\n",
    "    RandFlipd, RandSpatialCropd, Compose, SelectItemsd\n",
    ")\n",
    "\n",
    "from utils import *\n",
    "from models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Xg-nE1b3w-f",
    "outputId": "0490eeb3-e60d-4360-de3b-f29126c19f3f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 61\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# 4. MONAI CacheDatasets ------------------------------------\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# ── client-wise training sets ───────────────────────────────\u001b[39;00m\n\u001b[1;32m     60\u001b[0m CUT_OFF, FRAC, SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m---> 61\u001b[0m rng \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mRandom(SEED)\n\u001b[1;32m     63\u001b[0m train_datasets \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cid, subj_ids \u001b[38;5;129;01min\u001b[39;00m train_partitions\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    " \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# -----------------------------------------------------------\n",
    "# 0. paths & meta-data (unchanged) ---------------------------\n",
    "# -----------------------------------------------------------\n",
    "BRATS_DIR = \"/mnt/d/Datasets/FETS_data/MICCAI_FeTS2022_TrainingData\"\n",
    "CSV_PATH  = f\"{BRATS_DIR}/partitioning_1.csv\"\n",
    "MODALITIES = [\"flair\", \"t1\", \"t1ce\", \"t2\"]\n",
    "LABEL_KEY  = \"seg\"\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. read partition file  ➜  { id : [subjects] } ------------\n",
    "# -----------------------------------------------------------\n",
    "part_df       = pd.read_csv(CSV_PATH)\n",
    "partition_map = (\n",
    "    part_df.groupby(\"Partition_ID\")[\"Subject_ID\"]\n",
    "           .apply(list).to_dict()\n",
    ")                               # keys are 1 … 23\n",
    "\n",
    "# VAL_CENTRES = {18, 19, 20, 21, 22, 23}          # ← our hold-out set\n",
    "VAL_CENTRES = {22, 23}          # ← our sanity set\n",
    "\n",
    "# split once, reuse everywhere\n",
    "train_partitions = {cid: sids for cid, sids in partition_map.items()\n",
    "                    if cid not in VAL_CENTRES}\n",
    "val_subjects     = sum((partition_map[cid] for cid in VAL_CENTRES), [])\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. helper to build MONAI-style record dicts ----------------\n",
    "# -----------------------------------------------------------\n",
    "def build_records(subject_ids):\n",
    "    recs = []\n",
    "    for sid in subject_ids:\n",
    "        sdir = f\"{BRATS_DIR}/{sid}\"\n",
    "        rec  = {m: f\"{sdir}/{sid}_{m}.nii.gz\" for m in MODALITIES}\n",
    "        rec[\"seg\"] = f\"{sdir}/{sid}_{LABEL_KEY}.nii.gz\"\n",
    "        recs.append(rec)\n",
    "    return recs\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. transforms (unchanged) ---------------------------------\n",
    "# -----------------------------------------------------------\n",
    "IMG_KEYS = MODALITIES + [LABEL_KEY]\n",
    "train_tf = Compose([\n",
    "    LoadImaged(keys=IMG_KEYS), EnsureChannelFirstd(keys=IMG_KEYS),\n",
    "    Orientationd(keys=IMG_KEYS, axcodes=\"RAS\"),\n",
    "    ScaleIntensityd(keys=IMG_KEYS, minv=-1.0, maxv=1.0),\n",
    "    SelectItemsd(keys=IMG_KEYS),\n",
    "])\n",
    "val_tf = Compose([\n",
    "    LoadImaged(keys=IMG_KEYS), EnsureChannelFirstd(keys=IMG_KEYS),\n",
    "    Orientationd(keys=IMG_KEYS, axcodes=\"RAS\"),\n",
    "    ScaleIntensityd(keys=MODALITIES, minv=-1.0, maxv=1.0),   # masks untouched\n",
    "    SelectItemsd(keys=IMG_KEYS),\n",
    "])\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. MONAI CacheDatasets ------------------------------------\n",
    "# -----------------------------------------------------------\n",
    "# ── client-wise training sets ───────────────────────────────\n",
    "CUT_OFF, FRAC, SEED = 2, 0.1, 42\n",
    "rng = random.Random(SEED)\n",
    "\n",
    "train_datasets = {}\n",
    "for cid, subj_ids in train_partitions.items():\n",
    "    if cid > CUT_OFF:                                    # keep your cap\n",
    "        break\n",
    "    k = max(1, int(len(subj_ids) * FRAC))                # e.g. 30 %\n",
    "    sample_ids = rng.sample(subj_ids, k)\n",
    "    train_datasets[cid] = CacheDataset(\n",
    "        build_records(sample_ids), transform=train_tf, cache_rate=1\n",
    "    )\n",
    "\n",
    "# ── single validation dataset made from *all* val subjects ─\n",
    "test_dataset = CacheDataset(\n",
    "    build_records(val_subjects), transform=val_tf, cache_rate=1\n",
    ")\n",
    "\n",
    "print(\"train per-centre sizes:\", {k: len(v) for k, v in train_datasets.items()})\n",
    "print(\"validation size:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4.  Build per-client datasets & dataloaders\n",
    "# -----------------------------------------------------------\n",
    "# CUT_OFF    = 4  # How many sites to discard.\n",
    "# train_datasets = {}     # {client_id: monai CacheDataset}\n",
    "# for cid, subj_list in partition_map.items():\n",
    "#     if cid > CUT_OFF:\n",
    "#         print(f\"capping at {cid} for now\")\n",
    "#         break\n",
    "#     records = build_records(subj_list)\n",
    "#     train_datasets[cid] = CacheDataset(data=records, transform=train_tf, cache_rate=0.2)\n",
    "\n",
    "# --- SUBSAMPLE FOR DEV TESTING\n",
    "\n",
    "import random                    # NEW ─ reproducible subsampling\n",
    "FRAC = 0.3 #.10                      # 10 % of every client’s cases\n",
    "SEED = 42                        # set to None for pure randomness\n",
    "rng = random.Random(SEED)        # independent RNG so you don’t disturb numpy\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4.  Build per-client datasets & dataloaders\n",
    "# -----------------------------------------------------------\n",
    "CUT_OFF = 4\n",
    "train_datasets = {}\n",
    "\n",
    "for cid, subj_list in partition_map.items():\n",
    "    if cid > CUT_OFF:\n",
    "        print(f\"capping at {cid} for now\")\n",
    "        break\n",
    "\n",
    "    # ── pick 10 % of this client’s subjects ─────────────────\n",
    "    k = max(1, int(len(subj_list) * FRAC))        # always keep ≥1 case\n",
    "    sample_ids = rng.sample(subj_list, k)\n",
    "\n",
    "    # build dataset from that subset\n",
    "    records = build_records(sample_ids)\n",
    "    train_datasets[cid] = CacheDataset(data=records, transform=train_tf, cache_rate=1)\n",
    "\n",
    "print({cid: len(ds) for cid, ds in train_datasets.items()})\n",
    "# e.g. {1: 25, 2: 1, 3: 2, 4: 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 7.  Build test dataset & dataloader\n",
    "# -----------------------------------------------------------\n",
    "val_records  = build_val_records(VAL_DIR)\n",
    "test_dataset  = CacheDataset(data=val_records, transform=val_tf, cache_rate=1)\n",
    "# test_loader   = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrrjBXZg4rEN"
   },
   "outputs": [],
   "source": [
    "# def test_inference(model, test_dataset):\n",
    "#     # --------- INFERENCE FOR SEGMENTATION\n",
    "#     model.eval()\n",
    "#     total_dice = 0.0\n",
    "#     testloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in testloader:\n",
    "#             images = torch.stack([batch[k] for k in [\"flair\", \"t1\", \"t1ce\", \"t2\"]], dim=1).squeeze(2).to(device)  # (B, 4, D, H, W)\n",
    "#             labels = batch[\"seg\"].long().to(device)  # (B, 1, D, H, W)\n",
    "\n",
    "#             outputs = model(images)\n",
    "#             preds = torch.argmax(outputs, dim=1, keepdim=True)  # (B, 1, D, H, W)\n",
    "\n",
    "#             # Simple Dice calculation (per class optional)\n",
    "#             intersection = (preds == labels).sum().item()\n",
    "#             total = labels.numel()\n",
    "#             total_dice += 2.0 * intersection / (total + preds.numel())\n",
    "\n",
    "#     return total_dice / (len(testloader) + 0.0000001)\n",
    "\n",
    "def test_inference(model, test_dataset):\n",
    "    \"\"\"\n",
    "    Computes mean Dice across the three BraTS channels\n",
    "    (whole tumour, tumour-core, enhancing-core).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float   ─ mean Dice in the range [0, 1]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if len(test_dataset) == 0:\n",
    "        raise RuntimeError(f\"No validation cases found – check {VAL_DIR} and glob pattern.\")\n",
    "    loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    dsum = torch.zeros(3, device=device)\n",
    "    for batch in loader:\n",
    "        img = torch.cat([batch[k] for k in (\"flair\",\"t1\",\"t1ce\",\"t2\")],1).to(device)\n",
    "        raw = batch[\"seg\"].squeeze(1).cpu().numpy()\n",
    "        target = torch.tensor(preprocess_mask_labels(raw),\n",
    "                              dtype=torch.float32, device=device)\n",
    "        logits = model(img)\n",
    "        pred = torch.nn.functional.one_hot(\n",
    "                   torch.argmax(logits,1), num_classes=3\n",
    "               ).permute(0,4,1,2,3).float()\n",
    "        inter = 2*(pred*target).sum((2,3,4))\n",
    "        denom = (pred+target).sum((2,3,4))+1e-6\n",
    "        dsum += (inter/denom).squeeze(0)\n",
    "\n",
    "\n",
    "    return (dsum/len(loader)).mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTUtuKlu4Ddo"
   },
   "outputs": [],
   "source": [
    "# N = 10 #srch\n",
    "\n",
    "# after you build train_datasets\n",
    "idxs_users = np.array(sorted(train_datasets.keys()))\n",
    "N = len(idxs_users)\n",
    "\n",
    "# N = list(train_datasets.keys())[-1]\n",
    "print(f\"We got {N} clients\")\n",
    "local_bs = 2\n",
    "lr = 1e-4\n",
    "local_ep = 1\n",
    "EPOCHS = 1\n",
    "\n",
    "# noise_rates = np.linspace(0, 1, N, endpoint=False)\n",
    "# split_dset = mnist_iid(trainset, N)\n",
    "# user_groups = {i: 0 for i in range(1, N+1)}\n",
    "# noise_idx = {i: 0 for i in range(1, N+1)}\n",
    "# train_datasets = {i: 0 for i in range(1, N+1)}\n",
    "# for n in range(N):\n",
    "#     user_groups[n+1] = np.array(list(split_dset[n]), dtype=np.int)\n",
    "#     user_train_x, user_train_y = x_train[user_groups[n+1]], y_train[user_groups[n+1]]\n",
    "#     user_noisy_y, noise_idx[n+1] = noisify_MNIST(noise_rates[n], 'symmetric', user_train_x, user_train_y)\n",
    "    \n",
    "#     train_datasets[n+1] = CustomTensorDataset((user_train_x, user_noisy_y), transform_train)\n",
    "\n",
    "def copy_batchnorm_stats(subset_weights, global_model_state_dict):\n",
    "    for pair_1, pair_2 in zip(subset_weights.items(), global_model_state_dict.items()):\n",
    "        if ('running' in pair_1[0]) or ('batches' in pair_1[0]):\n",
    "            subset_weights[pair_1[0]] = global_model_state_dict[pair_1[0]]\n",
    "    \n",
    "    return subset_weights\n",
    "\n",
    "\n",
    "global_model = ResUNet3D(in_channels=4, out_channels=3).to(device) # Segmentation model used in FeTS\n",
    "\n",
    "global_model.to(device)\n",
    "global_model.train()\n",
    "\n",
    "global_weights = global_model.state_dict()\n",
    "powerset = list(powersettool(range(1, N+1)))[1:]          # discard ()\n",
    "# … the rest unchanged …\n",
    "\n",
    "\n",
    "# submodel_dict = {}\n",
    "# submodel_dict[()] = copy.deepcopy(global_model)\n",
    "# Change instead to storing the submodel weights to disk using torch.save\n",
    "import os\n",
    "submodel_dir = \"submodels\"\n",
    "os.makedirs(submodel_dir, exist_ok=True)\n",
    "submodel_file_template = os.path.join(submodel_dir, \"submodel_{}.pth\")\n",
    "global_model_path = os.path.join(submodel_dir, \"global_model.pth\")\n",
    "# Save the global model weights\n",
    "torch.save(global_weights, global_model_path)\n",
    "\n",
    "accuracy_dict = {}\n",
    "# accuracy_dict[()] = 0.0                                # utility of empty coalition\n",
    "shapley_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvXdFSu24Iq6",
    "outputId": "76c751e1-1bbf-49a6-9803-df0517c62df5"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Store initial single user submodels as the initial global model\n",
    "for user in range(1, N+1):\n",
    "    model_path = submodel_file_template.format(user)\n",
    "    # global_weights is already global_model.state_dict()  \n",
    "    torch.save(global_weights, model_path)\n",
    " \n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "print_every = 1\n",
    "\n",
    "idxs_users = np.arange(1, N+1)\n",
    "# total_data = sum(len(user_groups[i]) for i in range(1, N+1))\n",
    "# fraction = [len(user_groups[i])/total_data for i in range(1, N+1)]\n",
    "\n",
    "# ── collect dataset sizes ──────────────────────────────────────────────────\n",
    "# MONAI's CacheDataset inherits __len__, so `len(ds)` is cheap:\n",
    "sizes = {k: len(ds) for k, ds in train_datasets.items()}\n",
    "\n",
    "\n",
    "# ── total samples across all clients ───────────────────────────────────────\n",
    "total_data = sum(sizes.values())\n",
    "\n",
    "# ── FedAvg weight (a.k.a. fraction) for each client ────────────────────────\n",
    "# Keep the list in key order 1…N so it lines up with your loops later.\n",
    "fraction = [sizes[i] / total_data for i in range(1, N + 1)]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
    "    global_model.train()\n",
    "    for idx in tqdm(idxs_users):\n",
    "        trainloader = DataLoader(train_datasets[idx], batch_size=local_bs, shuffle=True,\n",
    "                                 num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "        local_trainer = LocalUpdateMONAI(\n",
    "            lr=lr,\n",
    "            local_ep=local_ep,\n",
    "            trainloader=trainloader,\n",
    "            img_keys=(\"flair\",\"t1\",\"t1ce\",\"t2\"),\n",
    "            label_key=\"seg\",\n",
    "        )\n",
    "        \n",
    "        local_model = copy.deepcopy(global_model)  # Ensure local_model is a fresh copy\n",
    "        w, loss = local_trainer.update_weights(model=local_model)\n",
    "        local_weights.append(w)\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "    del local_model  # Free memory after each client update\n",
    "    global_weights = average_weights(local_weights, fraction) # This operates on a list of state_dicts\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # calculate weight updates aka pseudo-gradients Delta_i = local_weight - old_global_weight\n",
    "    gradients = calculate_gradients(local_weights, global_model.state_dict())\n",
    "\n",
    "    # update the single user submodels stored on disk based on the pseudo-gradients\n",
    "    for i in range(1, N+1):\n",
    "        user_path = submodel_file_template.format(i)\n",
    "        prev_user_weights = torch.load(user_path)  # Load previous user weights\n",
    "        user_weights = update_weights_from_gradients(gradients[i-1], prev_user_weights)\n",
    "        \n",
    "        # unsure the following line is needed in this context\n",
    "        # subset_weights = copy_batchnorm_stats(subset_weights, global_model.state_dict())\n",
    "        \n",
    "        # Save the updated user weights back to disk\n",
    "        torch.save(user_weights, user_path)\n",
    "\n",
    "    # update the global model with the averaged weights\n",
    "    global_model.load_state_dict(global_weights)\n",
    "    global_model.eval()\n",
    "\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "        # print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
    "\n",
    "test_dsc = test_inference(global_model, test_dataset)\n",
    "print(f' \\n Results after {EPOCHS} global rounds of training:')\n",
    "print(\"|---- Test DSC: {:.2f}%\".format(100*test_dsc))\n",
    "\n",
    "accuracy_dict[powerset[-1]] = test_dsc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ADJUSTED-OR APPROX\n",
    "for subset in powerset[:-1]: \n",
    "    if len(subset) > 1:\n",
    "        # calculate the average of the subset of weights from list of all the weights in the subset\n",
    "        subset_weights = average_weights([torch.load(submodel_file_template.format(user)) for user in subset], [fraction[i-1] for i in subset])\n",
    "    elif len(subset) == 1:\n",
    "        # for single user submodels, just load the weights from disk, no need to average\n",
    "        subset_weights = torch.load(submodel_file_template.format(subset[0]))\n",
    "        # need to make a model from the averaged weights to test it\n",
    "    else:\n",
    "        continue\n",
    "    submodel = ResUNet3D(in_channels=4, out_channels=3).to(device)\n",
    "    submodel.load_state_dict(subset_weights)   \n",
    "    test_dsc = test_inference(submodel,test_dataset)\n",
    "    print(f' \\n Results after {EPOCHS} global rounds of training (for OR): ')\n",
    "    print(\"|---- Test DSC for {}: {:.2f}%\".format(subset, 100*test_dsc))\n",
    "    accuracy_dict[subset] = test_dsc\n",
    "    del submodel  # Free memory after each subset test\n",
    "\n",
    "trainTime = time.time() - start_time\n",
    "start_time = time.time()\n",
    "shapley_dict = shapley(accuracy_dict, N)\n",
    "shapTime = time.time() - start_time\n",
    "start_time = time.time()\n",
    "lc_dict = least_core(accuracy_dict, N)\n",
    "LCTime = time.time() - start_time\n",
    "totalShapTime = trainTime + shapTime\n",
    "totalLCTime = trainTime + LCTime\n",
    "print(f'\\n ACCURACY: {accuracy_dict[powerset[-1]]}')\n",
    "print('\\n Total Time Shapley: {0:0.4f}'.format(totalShapTime))\n",
    "print('\\n Total Time LC: {0:0.4f}'.format(totalLCTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapley allocation:\")\n",
    "for cid, phi in shapley_dict.items():\n",
    "    print(f\" client {cid}: {phi:.4f}\")\n",
    "\n",
    "print(\"\\nLeast-Core allocation:\")\n",
    "for var in lc_dict.variables():\n",
    "    if var.name.startswith(\"x(\"):\n",
    "        print(f\" client {var.name[2:-1]}: {var.value():.4f}\")\n",
    "print(f\" e (slack): {lc_dict.variablesDict()['e'].value():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOBqzOdQRfIe"
   },
   "outputs": [],
   "source": [
    "def stats(vector):\n",
    "    n = len(vector)\n",
    "    egal = np.array([1/n for i in range(n)])\n",
    "    normalised = np.array(vector / vector.sum())\n",
    "    msg = f'Original vector: {vector}\\n'\n",
    "    msg += f'Normalised vector: {normalised}\\n'\n",
    "    msg += f'Max Dif: {normalised.max()-normalised.min()}\\n'\n",
    "    msg += f'Distance: {np.linalg.norm(normalised-egal)}\\n'\n",
    "\n",
    "    msg += f'Budget: {vector.sum()}\\n'\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWyxrrCUwOxO",
    "outputId": "50b2f298-6572-4de7-8942-4065a3e2b0c8"
   },
   "outputs": [],
   "source": [
    "stats(np.array(list(shapley_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZ3ZJVjFJA1c",
    "outputId": "86f4a6b1-a728-45ea-83d6-7d9913b59823"
   },
   "outputs": [],
   "source": [
    "stats(np.array([i.value() for i in lc_dict.variables()])[1:])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMNKnjfLFi/+UJW/ZI4jUCD",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ISO CIFAR10 OR FINAL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
