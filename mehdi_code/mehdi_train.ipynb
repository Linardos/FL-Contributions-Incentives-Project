{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8937d7ff-6be2-4d2b-b03e-1be0d161ec42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/nnunet/preprocessed /mnt/d/nnunet/results /mnt/d/nnunet/raw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"nnUNet_raw\"] = \"/mnt/d/nnunet/raw\"\n",
    "os.environ[\"nnUNet_preprocessed\"] = \"/mnt/d/nnunet/preprocessed\"\n",
    "os.environ[\"nnUNet_results\"] = \"/mnt/d/nnunet/results\"\n",
    "\n",
    "from nnunetv2.paths import nnUNet_preprocessed, nnUNet_results, nnUNet_raw\n",
    "\n",
    "print(nnUNet_preprocessed, nnUNet_results, nnUNet_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc330824-94fb-4b31-8f93-25475ce53e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "from nnunet_api import NnUnetApi\n",
    "from tools.data_reformat import data_prepare\n",
    "from tools.json_pickle_stuff import copy_plans_json\n",
    "\n",
    "# =================================================================================\n",
    "# 1. Configuration\n",
    "# =================================================================================\n",
    "# --- Define the IDs and paths for your datasets and models ---\n",
    "\n",
    "# ABS path to the raw data directory: each subject has its own folder with BraTS namind convention\n",
    "RAW_DATA_PATH = \"/mnt/d/Datasets/mehdi_code_test/sample_data\" #\"PATH_TO_SAMPLE_RAW_DATA\"\n",
    "\n",
    "# The Dataset ID of the model you want to use for pre-training.\n",
    "# This model's architecture and plans will be transferred to your new dataset.\n",
    "PRETRAINED_DATASET_ID = 770\n",
    "\n",
    "# The Dataset ID of your new dataset that you want to fine-tune on.\n",
    "# Make sure you have already converted this dataset to the nnU-Net format.\n",
    "FINETUNE_DATASET_ID = 666\n",
    "\n",
    "# An identifier for the new plans that will be created for your fine-tuning dataset.\n",
    "# It's good practice to give it a descriptive name.\n",
    "FINETUNE_PLANS_ID = 'nnUNetPlans_finetune_from_brats'\n",
    "\n",
    "# The full path to the pre-trained model's checkpoint file.\n",
    "# This file contains the weights that will be used to initialize your new model.\n",
    "PRETRAINED_CHECKPOINT_PATH = os.path.join(nnUNet_results, \"Dataset770_BraTSGLIPreCropRegion/nnUNetTrainer__nnUNetResEncUNetPlans__3d_fullres/fold_0/checkpoint_final.pth\")\n",
    "\n",
    "# The GPU device to use for training.\n",
    "# DEVICE = torch.device('cuda')\n",
    "\n",
    "# Fold number you want to train (on the fine tunning dataset)\n",
    "FOLD = 0\n",
    "\n",
    "# NUMBER of epochs to train the model\n",
    "N_EPOCHS = 3\n",
    "\n",
    "# Initial Learning Rate for model training\n",
    "INIT_LR = 1e-3\n",
    "\n",
    "# ABS Path to the folder where testing data is located\n",
    "INPUT_FOLDER_INFER = \"/mnt/d/Datasets/mehdi_code_test/sample_test_decathlon\" #'PATH_TO_TESTING_DATA'\n",
    "\n",
    "# ABS Path to the folder where the results of inference will be saved\n",
    "OUTPUT_FOLDER_INFER_FINETUNE = './test_ftune' #'PATH_TO_SAVE_RESULTS_FINETUNE'\n",
    "OUTPUT_FOLDER_INFER_PRETRAINED = './test_pretrain' #'PATH_TO_SAVE_RESULTS_PRETRAINED'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dccfdb0-c049-42f8-9300-ba88362a909b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0: Running Data Preparation script...\n",
      "--------\n",
      "data reformat in process for case 1 out of 10 ...\n",
      "data reformat in process for case 2 out of 10 ...\n",
      "data reformat in process for case 3 out of 10 ...\n",
      "data reformat in process for case 4 out of 10 ...\n",
      "data reformat in process for case 5 out of 10 ...\n",
      "data reformat in process for case 6 out of 10 ...\n",
      "data reformat in process for case 7 out of 10 ...\n",
      "data reformat in process for case 8 out of 10 ...\n",
      "data reformat in process for case 9 out of 10 ...\n",
      "data reformat in process for case 10 out of 10 ...\n",
      "--------\n",
      "All files were reformated, ready for segmentation!\n"
     ]
    }
   ],
   "source": [
    "# ## --- Step 00: Preparing the raw dataset into Decathlon format\n",
    "print(\"\\nStep 0: Running Data Preparation script...\")\n",
    "DST_DATA_NAME = \"Dataset\"+str(FINETUNE_DATASET_ID)+\"_finetune_Decathlon\"\n",
    "data_prepare(RAW_DATA_PATH, os.path.join(nnUNet_raw, DST_DATA_NAME))\n",
    "n_case=len(os.listdir(os.path.join(nnUNet_raw, DST_DATA_NAME,\"labelsTr\")))\n",
    "copy_plans_json(\"./dataset.json\", os.path.join(nnUNet_raw, DST_DATA_NAME), n_case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094a821d-219c-419e-a023-8dee260f33d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Extracting fingerprint for Dataset 666...\n",
      "Dataset666_finetune_Decathlon\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Fingerprint extracted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =================================================================================\n",
    "# 2. Initialize the API\n",
    "# =================================================================================\n",
    "api = NnUnetApi()\n",
    "## --- Step 1: Extract the Fingerprint for Your New Dataset ---\n",
    "## This step analyzes the properties of your new dataset (image sizes, spacings, etc.)\n",
    "## and creates a \"fingerprint\" file. This is a prerequisite for any planning.\n",
    "## This wraps: nnUNetv2_extract_fingerprint CLI\n",
    "print(f\"Step 1: Extracting fingerprint for Dataset {FINETUNE_DATASET_ID}...\")\n",
    "api.extract_fingerprint(\n",
    "    finetune_dataset_id=FINETUNE_DATASET_ID\n",
    ")\n",
    "print(\"-> Fingerprint extracted.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67193b95-789c-4dfa-bb2f-c323ce6f1af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Applying plans from Dataset 770 to Dataset 666...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Cannot move plans because preprocessed directory of source dataset is missing. Run nnUNetv2_plan_and_preprocess for source dataset first!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- Step 2: Load and Apply the Pre-trained Model's Plans ---\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This is the key step for aligning architectures. It takes the plans\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# (network topology, patch size, normalization, etc.) from the pre-trained\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model and applies them to your new dataset's fingerprint, creating a new\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# plan file specifically for fine-tuning.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# This wraps: nnUNetv2_move_plans_between_datasets CLI\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 2: Applying plans from Dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPRETRAINED_DATASET_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to Dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINETUNE_DATASET_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_pretrained_plans\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_dataset_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPRETRAINED_DATASET_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinetune_dataset_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFINETUNE_DATASET_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinetune_plans_identifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFINETUNE_PLANS_ID\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-> Plans applied. New plans identifier is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINETUNE_PLANS_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/FL-Contributions-Incentives-Project/gli25/nnunet_api.py:82\u001b[0m, in \u001b[0;36mNnUnetApi.apply_pretrained_plans\u001b[0;34m(self, pretrained_dataset_id, finetune_dataset_id, pretrained_plans_identifier, finetune_plans_identifier)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Simulates: nnUNetv2_move_plans_between_datasets -s SOURCE_ID -t TARGET_ID -sp SOURCE_PLANS -tp TARGET_PLANS\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-s\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(pretrained_dataset_id), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-t\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(finetune_dataset_id), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-sp\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained_plans_identifier, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-tp\u001b[39m\u001b[38;5;124m'\u001b[39m, finetune_plans_identifier]\n\u001b[0;32m---> 82\u001b[0m     \u001b[43mentry_point_move_plans_between_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m=\u001b[39m original_argv\n",
      "File \u001b[0;32m~/nnUNet/nnunetv2/experiment_planning/plans_for_pretraining/move_plans_between_datasets.py:78\u001b[0m, in \u001b[0;36mentry_point_move_plans_between_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-tp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     74\u001b[0m                     help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget plans identifier. Default is None meaning the source plans identifier will \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     75\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbe kept. Not recommended if the source plans identifier is a default nnU-Net identifier \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     76\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuch as nnUNetPlans!!!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     77\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args()\n\u001b[0;32m---> 78\u001b[0m \u001b[43mmove_plans_between_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nnUNet/nnunetv2/experiment_planning/plans_for_pretraining/move_plans_between_datasets.py:24\u001b[0m, in \u001b[0;36mmove_plans_between_datasets\u001b[0;34m(source_dataset_name_or_id, target_dataset_name_or_id, source_plans_identifier, target_plans_identifier)\u001b[0m\n\u001b[1;32m     21\u001b[0m     target_plans_identifier \u001b[38;5;241m=\u001b[39m source_plans_identifier\n\u001b[1;32m     23\u001b[0m source_folder \u001b[38;5;241m=\u001b[39m join(nnUNet_preprocessed, source_dataset_name)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m isdir(source_folder), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot move plans because preprocessed directory of source dataset is missing. \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     25\u001b[0m                              \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun nnUNetv2_plan_and_preprocess for source dataset first!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m source_plans_file \u001b[38;5;241m=\u001b[39m join(source_folder, source_plans_identifier \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m isfile(source_plans_file), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource plans are missing. Run the corresponding experiment planning first! \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     29\u001b[0m                                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_plans_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Cannot move plans because preprocessed directory of source dataset is missing. Run nnUNetv2_plan_and_preprocess for source dataset first!"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Load and Apply the Pre-trained Model's Plans ---\n",
    "# This is the key step for aligning architectures. It takes the plans\n",
    "# (network topology, patch size, normalization, etc.) from the pre-trained\n",
    "# model and applies them to your new dataset's fingerprint, creating a new\n",
    "# plan file specifically for fine-tuning.\n",
    "# This wraps: nnUNetv2_move_plans_between_datasets CLI\n",
    "print(f\"Step 2: Applying plans from Dataset {PRETRAINED_DATASET_ID} to Dataset {FINETUNE_DATASET_ID}...\")\n",
    "api.apply_pretrained_plans(\n",
    "    pretrained_dataset_id=PRETRAINED_DATASET_ID,\n",
    "    finetune_dataset_id=FINETUNE_DATASET_ID,\n",
    "    finetune_plans_identifier=FINETUNE_PLANS_ID\n",
    ")\n",
    "print(f\"-> Plans applied. New plans identifier is '{FINETUNE_PLANS_ID}'.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
