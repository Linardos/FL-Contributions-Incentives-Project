{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41873405-4d1f-4ff4-8139-ba09cf6b3f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Done] Staged 1864 files into: /mnt/d/Datasets/FETS_data/INPUT_IMAGES_FOR_NNUNET\n",
      "[Check] Subjects staged: 1 (expected 4 files each)\n",
      "[Warn] Off-count subjects: {'FeTS2022': 1864}\n"
     ]
    }
   ],
   "source": [
    "# ==== Stage Decathlon-style files for nnU-Net inference (fixed) ====\n",
    "\n",
    "import os, shutil, errno\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BRATS_DIR = \"/mnt/d/Datasets/FETS_data/MICCAI_FeTS2022_TrainingData\"\n",
    "CSV_PATH  = f\"{BRATS_DIR}/partitioning_1.csv\"\n",
    "MODALITIES = [\"flair\", \"t1\", \"t1ce\", \"t2\"]\n",
    "VAL_CENTRES = {18, 19, 20, 21, 22, 23}\n",
    "\n",
    "# Build val_subjects if not already present\n",
    "if \"val_subjects\" not in globals():\n",
    "    part_df = pd.read_csv(CSV_PATH)\n",
    "    partition_map = part_df.groupby(\"Partition_ID\")[\"Subject_ID\"].apply(list).to_dict()\n",
    "    val_subjects = sum((partition_map[cid] for cid in VAL_CENTRES), [])\n",
    "\n",
    "# nnU-Net suffix order: 0=t1n, 1=t1c, 2=t2w, 3=t2f\n",
    "SUFFIX_MAP = {\n",
    "    \"t1\":   \"_0000\",\n",
    "    \"t1ce\": \"_0001\",\n",
    "    \"t2\":   \"_0002\",\n",
    "    \"flair\":\"_0003\",\n",
    "}\n",
    "\n",
    "# You can keep your custom path; nnU-Net just needs a folder of cases\n",
    "INPUT_FOLDER_INFER = Path(\"/mnt/d/Datasets/FETS_data/INPUT_IMAGES_FOR_NNUNET\")\n",
    "INPUT_FOLDER_INFER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _safe_symlink_or_copy(src: Path, dst: Path):\n",
    "    try:\n",
    "        if dst.exists():\n",
    "            return\n",
    "        os.symlink(src, dst)\n",
    "    except OSError as e:\n",
    "        # cross-device or permission issues → copy instead\n",
    "        if e.errno in (errno.EPERM, errno.EACCES, errno.EXDEV, errno.EOPNOTSUPP):\n",
    "            shutil.copy2(src, dst)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "missing = []\n",
    "made = 0\n",
    "\n",
    "for sid in val_subjects:\n",
    "    sdir = Path(BRATS_DIR) / sid  # e.g., .../FeTS2022_00000\n",
    "    for m in MODALITIES:\n",
    "        src = sdir / f\"{sid}_{m}.nii.gz\"\n",
    "        if not src.exists():\n",
    "            missing.append(str(src))\n",
    "            continue\n",
    "        dst = INPUT_FOLDER_INFER / f\"{sid}{SUFFIX_MAP[m]}.nii.gz\"\n",
    "        _safe_symlink_or_copy(src, dst)\n",
    "        made += 1\n",
    "\n",
    "print(f\"[Done] Staged {made} files into: {INPUT_FOLDER_INFER}\")\n",
    "if missing:\n",
    "    print(f\"[Warn] Missing {len(missing)} files; first few:\\n  - \" + \"\\n  - \".join(missing[:8]))\n",
    "\n",
    "# Quick sanity: should be exactly 4 files per subject\n",
    "from collections import Counter\n",
    "counts = Counter(p.name.split(\"_\")[0] for p in INPUT_FOLDER_INFER.glob(\"*.nii.gz\"))\n",
    "bad = {k:v for k,v in counts.items() if v != 4}\n",
    "print(f\"[Check] Subjects staged: {len(counts)} (expected 4 files each)\")\n",
    "if bad:\n",
    "    print(\"[Warn] Off-count subjects:\", bad)\n",
    "else:\n",
    "    print(\"[OK] All subjects have 4 modalities.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f326b2f8-b4c7-466e-8444-2eea2d0bbd84",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/tmp/nnunet_results/Dataset770_BraTSGLIPreCropRegion/nnUNetTrainer__nnUNetResEncUNetPlans__3d_fullres/dataset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m Path(OUTPUT_FOLDER_PREDS)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m api \u001b[38;5;241m=\u001b[39m NnUnetApi()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINPUT_FOLDER_INFER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_FOLDER_PREDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name_or_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataset770_BraTSGLIPreCropRegion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplans_identifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnnUNetResEncUNetPlans\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m3d_fullres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrote preds to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, OUTPUT_FOLDER_PREDS)\n",
      "File \u001b[0;32m~/FL-Contributions-Incentives-Project/gli25/nnunet_api.py:33\u001b[0m, in \u001b[0;36mNnUnetApi.predict\u001b[0;34m(self, input_folder, output_folder, dataset_name_or_id, configuration, folds, checkpoint_name, plans_identifier)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# The model folder path is constructed using the plans_identifier.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m model_folder \u001b[38;5;241m=\u001b[39m join(nnUNet_results, \u001b[38;5;28mstr\u001b[39m(dataset_name_or_id), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnnUNetTrainer__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplans_identifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfiguration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_from_trained_model_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Create a list of lists, where each sub-list contains all modalities of a single case\u001b[39;00m\n\u001b[1;32m     40\u001b[0m input_files \u001b[38;5;241m=\u001b[39m subfiles(input_folder, suffix\u001b[38;5;241m=\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mdataset_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_ending\u001b[39m\u001b[38;5;124m'\u001b[39m], join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/nnUNet/nnunetv2/inference/predict_from_raw_data.py:76\u001b[0m, in \u001b[0;36mnnUNetPredictor.initialize_from_trained_model_folder\u001b[0;34m(self, model_training_output_dir, use_folds, checkpoint_name)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_folds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     use_folds \u001b[38;5;241m=\u001b[39m nnUNetPredictor\u001b[38;5;241m.\u001b[39mauto_detect_available_folds(model_training_output_dir, checkpoint_name)\n\u001b[0;32m---> 76\u001b[0m dataset_json \u001b[38;5;241m=\u001b[39m \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_training_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m plans \u001b[38;5;241m=\u001b[39m load_json(join(model_training_output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplans.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     78\u001b[0m plans_manager \u001b[38;5;241m=\u001b[39m PlansManager(plans)\n",
      "File \u001b[0;32m~/miniconda3/envs/m_quant_py310/lib/python3.10/site-packages/batchgenerators/utilities/file_and_folder_operations.py:103\u001b[0m, in \u001b[0;36mload_json\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_json\u001b[39m(file: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    104\u001b[0m         a \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/tmp/nnunet_results/Dataset770_BraTSGLIPreCropRegion/nnUNetTrainer__nnUNetResEncUNetPlans__3d_fullres/dataset.json'"
     ]
    }
   ],
   "source": [
    "# -- One-time (in this kernel) --\n",
    "import os\n",
    "os.environ[\"nnUNet_raw\"] = \"/mnt/tmp/nnunet_raw\"\n",
    "os.environ[\"nnUNet_preprocessed\"] = \"/mnt/tmp/nnunet_preprocessed\"\n",
    "os.environ[\"nnUNet_results\"] = \"/mnt/tmp/nnunet_results\"\n",
    "\n",
    "# Put these assets as per README before you run predict:\n",
    "# nnUNet_results/Dataset770_BraTSGLIPreCropRegion/nnUNetTrainer__nnUNetResEncUNetPlans__3d_fullres/fold_0/checkpoint_final.pth\n",
    "# nnUNet_preprocessed/Dataset770_BraTSGLIPreCropRegion/nnUNetResEncUNetPlans.json\n",
    "\n",
    "from pathlib import Path\n",
    "from nnunet_api import NnUnetApi\n",
    "\n",
    "INPUT_FOLDER_INFER  = \"/mnt/d/Datasets/FETS_data/INPUT_IMAGES_FOR_NNUNET\"\n",
    "OUTPUT_FOLDER_PREDS = \"/mnt/d/Datasets/FETS_data/NNUNET_PREDS_770\"\n",
    "Path(OUTPUT_FOLDER_PREDS).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "api = NnUnetApi()\n",
    "api.predict(\n",
    "    input_folder=INPUT_FOLDER_INFER,\n",
    "    output_folder=OUTPUT_FOLDER_PREDS,\n",
    "    dataset_name_or_id=\"Dataset770_BraTSGLIPreCropRegion\",\n",
    "    plans_identifier=\"nnUNetResEncUNetPlans\",\n",
    "    configuration=\"3d_fullres\",\n",
    "    folds=[0]\n",
    ")\n",
    "print(\"Wrote preds to:\", OUTPUT_FOLDER_PREDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c777f52-74d5-4956-a740-b2e7aceeb5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def _label_to_onehot_3(raw_mask_np):\n",
    "# raw_mask_np is the integer mask from nnU-Net (0 background, 1/2/3 tumor labels)\n",
    "whole = (raw_mask_np == 1) | (raw_mask_np == 2) | (raw_mask_np == 3)\n",
    "tumor_core = (raw_mask_np == 1) | (raw_mask_np == 3)\n",
    "enhancing = (raw_mask_np == 3)\n",
    "onehot = np.stack([whole, tumor_core, enhancing], axis=0).astype(np.float32)\n",
    "return torch.from_numpy(onehot) # [3, D, H, W]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def dice3_from_preds_dir(preds_dir, test_dataset, device):\n",
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "dsum = torch.zeros(3, device=device)\n",
    "\n",
    "\n",
    "# Build a map from subject id -> pred path\n",
    "pred_map = {p.name.replace(\".nii.gz\", \"\"): str(p)\n",
    "for p in Path(preds_dir).glob(\"*.nii.gz\")}\n",
    "\n",
    "\n",
    "for batch in loader:\n",
    "# Subject id recovered from one of the modality file names in your dataset\n",
    "# E.g., \"BraTS-GLI-00160-000_0000.nii.gz\" => case id \"BraTS-GLI-00160-000\"\n",
    "flair_path = batch[\"flair_meta_dict\"][\"filename_or_obj\"][0]\n",
    "sid = Path(flair_path).name.split(\"_\")[0]\n",
    "pred_path = pred_map.get(sid)\n",
    "if pred_path is None:\n",
    "raise FileNotFoundError(f\"No prediction found for {sid}\")\n",
    "\n",
    "\n",
    "# target from your labels\n",
    "raw_gt = batch[\"seg\"].squeeze(1).cpu().numpy()[0]\n",
    "target = _label_to_onehot_3(raw_gt).to(device).unsqueeze(0) # [1,3,D,H,W]\n",
    "\n",
    "\n",
    "# predicted mask -> onehot 3‑channel\n",
    "raw_pred = nib.load(pred_path).get_fdata().astype(np.int16)\n",
    "pred = _label_to_onehot_3(raw_pred).to(device).unsqueeze(0)\n",
    "\n",
    "\n",
    "inter = 2*(pred*target).sum((2,3,4))\n",
    "denom = (pred+target).sum((2,3,4))+1e-6\n",
    "dsum += (inter/denom).squeeze(0)\n",
    "\n",
    "\n",
    "return (dsum/len(loader)).mean().item()\n",
    "\n",
    "\n",
    "# Usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dice = dice3_from_preds_dir(OUTPUT_FOLDER_PRETRAINED, test_dataset, device)\n",
    "print(\"Mean Dice (whole/core/enhancing averaged):\", dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f088243d-83d9-402a-9347-2f642554a301",
   "metadata": {},
   "source": [
    "# No NNUnet yet  under here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b79e98-f0e7-439c-8ab9-78665f77cd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/locolinux2/miniconda3/envs/m_quant/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 511/511 [12:28<00:00,  1.46s/it]\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:07<00:00,  1.30s/it]\n",
      "Loading dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:19<00:00,  1.32s/it]\n",
      "Loading dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [01:08<00:00,  1.46s/it]\n",
      "Loading dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:30<00:00,  1.39s/it]\n",
      "Loading dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:51<00:00,  1.52s/it]\n",
      "Loading dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:15<00:00,  1.25s/it]\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:12<00:00,  1.51s/it]\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.42s/it]\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:11<00:00,  1.45s/it]\n",
      "Loading dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:19<00:00,  1.42s/it]\n",
      "Loading dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:14<00:00,  1.34s/it]\n",
      "Loading dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:46<00:00,  1.34s/it]\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:08<00:00,  1.41s/it]\n",
      "Loading dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:20<00:00,  1.55s/it]\n",
      "Loading dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [01:01<00:00,  2.06s/it]\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:13<00:00,  1.55s/it]\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 466/466 [14:44<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train per-centre sizes: {1: 511, 2: 6, 3: 15, 4: 47, 5: 22, 6: 34, 7: 12, 8: 8, 9: 4, 10: 8, 11: 14, 12: 11, 13: 35, 14: 6, 15: 13, 16: 30, 17: 9}\n",
      "validation size: 466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, copy, time, random, torch, numpy as np                                 # ← your own\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from monai.data import CacheDataset\n",
    "import glob, nibabel as nib, pandas as pd\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.transforms import (\n",
    "    LoadImaged, EnsureChannelFirstd, Orientationd, ScaleIntensityd,\n",
    "    RandFlipd, RandSpatialCropd, Compose, SelectItemsd\n",
    ")\n",
    "\n",
    "from utils import *\n",
    "from models import *\n",
    "  \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# -----------------------------------------------------------\n",
    "# 0. paths & meta-data (unchanged) ---------------------------\n",
    "# -----------------------------------------------------------\n",
    "BRATS_DIR = \"/mnt/d/Datasets/FETS_data/MICCAI_FeTS2022_TrainingData\"\n",
    "CSV_PATH  = f\"{BRATS_DIR}/partitioning_1.csv\"\n",
    "MODALITIES = [\"flair\", \"t1\", \"t1ce\", \"t2\"]\n",
    "LABEL_KEY  = \"seg\"\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. read partition file  ➜  { id : [subjects] } ------------\n",
    "# -----------------------------------------------------------\n",
    "part_df       = pd.read_csv(CSV_PATH)\n",
    "partition_map = (\n",
    "    part_df.groupby(\"Partition_ID\")[\"Subject_ID\"]\n",
    "           .apply(list).to_dict()\n",
    ")                               # keys are 1 … 23\n",
    "\n",
    "VAL_CENTRES = {18, 19, 20, 21, 22, 23}          # ← our hold-out set\n",
    "# VAL_CENTRES = {22, 23}          # ← our sanity set\n",
    "\n",
    "# split once, reuse everywhere\n",
    "train_partitions = {cid: sids for cid, sids in partition_map.items()\n",
    "                    if cid not in VAL_CENTRES}\n",
    "val_subjects     = sum((partition_map[cid] for cid in VAL_CENTRES), [])\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. helper to build MONAI-style record dicts ----------------\n",
    "# -----------------------------------------------------------\n",
    "def build_records(subject_ids):\n",
    "    recs = []\n",
    "    for sid in subject_ids:\n",
    "        sdir = f\"{BRATS_DIR}/{sid}\"\n",
    "        rec  = {m: f\"{sdir}/{sid}_{m}.nii.gz\" for m in MODALITIES}\n",
    "        rec[\"seg\"] = f\"{sdir}/{sid}_{LABEL_KEY}.nii.gz\"\n",
    "        recs.append(rec)\n",
    "    return recs\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. transforms (unchanged) ---------------------------------\n",
    "# -----------------------------------------------------------\n",
    "IMG_KEYS = MODALITIES + [LABEL_KEY]\n",
    "train_tf = Compose([\n",
    "    LoadImaged(keys=IMG_KEYS), EnsureChannelFirstd(keys=IMG_KEYS),\n",
    "    Orientationd(keys=IMG_KEYS, axcodes=\"RAS\"),\n",
    "    ScaleIntensityd(keys=IMG_KEYS, minv=-1.0, maxv=1.0),\n",
    "    SelectItemsd(keys=IMG_KEYS),\n",
    "])\n",
    "val_tf = Compose([\n",
    "    LoadImaged(keys=IMG_KEYS), EnsureChannelFirstd(keys=IMG_KEYS),\n",
    "    Orientationd(keys=IMG_KEYS, axcodes=\"RAS\"),\n",
    "    ScaleIntensityd(keys=MODALITIES, minv=-1.0, maxv=1.0),   # masks untouched\n",
    "    SelectItemsd(keys=IMG_KEYS),\n",
    "])\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. MONAI CacheDatasets ------------------------------------\n",
    "# -----------------------------------------------------------\n",
    "# ── client-wise training sets ───────────────────────────────\n",
    "CUT_OFF, FRAC, SEED = 18, 1, 42\n",
    "rng = random.Random(SEED)\n",
    "\n",
    "train_datasets = {}\n",
    "for cid, subj_ids in train_partitions.items():\n",
    "    if cid > CUT_OFF:                                    # keep your cap\n",
    "        break\n",
    "    k = max(1, int(len(subj_ids) * FRAC))                # e.g. 30 %\n",
    "    sample_ids = rng.sample(subj_ids, k)\n",
    "    train_datasets[cid] = CacheDataset(\n",
    "        build_records(sample_ids), transform=train_tf, cache_rate=1\n",
    "    )\n",
    "\n",
    "# ── single validation dataset made from *all* val subjects ─\n",
    "test_dataset = CacheDataset(\n",
    "    build_records(val_subjects), transform=val_tf, cache_rate=1\n",
    ")\n",
    "\n",
    "print(\"train per-centre sizes:\", {k: len(v) for k, v in train_datasets.items()})\n",
    "print(\"validation size:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc046f34-4c0a-474c-8016-99d86045c431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation size: 466\n"
     ]
    }
   ],
   "source": [
    "print(\"validation size:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "381f8fa9-30a1-4942-b36b-0c60a58b22f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([1, 3, 96, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "from seg_models import *      # adjust path / PYTHONPATH\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = ResUNet3D(4,3).to(device)\n",
    "\n",
    "# quick dummy forward\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 4, 96, 96, 96, device=device)\n",
    "    logits = model(dummy)\n",
    "print(\"logits shape:\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cf826f6-cd7b-44b6-9c2a-4b81e6e04356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice before any training: 0.011921419762074947\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# 2. helper: Dice on whole/TC/ET averaged to a scalar --------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def dice3(model, test_dataset):\n",
    "    model.eval()\n",
    "    if len(test_dataset) == 0:\n",
    "        raise RuntimeError(f\"No validation cases found – check {VAL_DIR} and glob pattern.\")\n",
    "    loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    dsum = torch.zeros(3, device=device)\n",
    "    for batch in loader:\n",
    "        img = torch.cat([batch[k] for k in (\"flair\",\"t1\",\"t1ce\",\"t2\")],1).to(device)\n",
    "        raw = batch[\"seg\"].squeeze(1).cpu().numpy()\n",
    "        target = torch.tensor(preprocess_mask_labels(raw),\n",
    "                              dtype=torch.float32, device=device)\n",
    "        logits = model(img)\n",
    "        pred = torch.nn.functional.one_hot(\n",
    "                   torch.argmax(logits,1), num_classes=3\n",
    "               ).permute(0,4,1,2,3).float()\n",
    "        inter = 2*(pred*target).sum((2,3,4))\n",
    "        denom = (pred+target).sum((2,3,4))+1e-6\n",
    "        dsum += (inter/denom).squeeze(0)\n",
    "\n",
    "\n",
    "    return (dsum/len(loader)).mean().item()\n",
    "\n",
    "global_model = ResUNet3D(4,3).to(device)\n",
    "print(\"Dice before any training:\", dice3(global_model, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3eb953-eef2-4c1f-a07b-6345d342c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange   # trange == tqdm(range())\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# 1. one-client update (returns weights + mean loss)          │\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def local_train(model, loader, lr=1e-4, epochs=1):\n",
    "    crit = BCEDiceLoss().to(device)\n",
    "    opt  = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        running = 0.0\n",
    "        for img_dict in loader:\n",
    "            img = torch.cat([img_dict[k] for k in (\"flair\", \"t1\", \"t1ce\", \"t2\")], 1).to(device)\n",
    "            msk = preprocess_mask_labels(img_dict[\"seg\"].squeeze(1).numpy())\n",
    "            msk = torch.tensor(msk, dtype=torch.float32, device=device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(img), msk)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            running += loss.item()\n",
    "        losses.append(running / len(loader))          # epoch-mean\n",
    "\n",
    "    return model.state_dict(), np.mean(losses)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# 2. FedAvg training loop (simple tqdm + clean prints)        │\n",
    "# ────────────────────────────────────────────────────────────\n",
    "EPOCHS, LOCAL_EPOCHS, LR, BATCH = 50, 1, 1e-4, 1          # dial as needed\n",
    "idxs_users = list(train_datasets.keys())\n",
    "sizes      = {k: len(ds) for k, ds in train_datasets.items()}\n",
    "fractions  = [sizes[k] / sum(sizes.values()) for k in idxs_users]\n",
    "\n",
    "global_model = ResUNet3D(4, 3).to(device)\n",
    "print(f\"Dice before training: {dice3(global_model, test_dataset):.4f}\")\n",
    "\n",
    "for rnd in trange(1, EPOCHS + 1, desc=\"Global rounds\"):\n",
    "    local_weights, client_losses = [], []\n",
    "\n",
    "    for cid in tqdm(idxs_users, desc=\" clients\", leave=False):\n",
    "        loader = DataLoader(train_datasets[cid], batch_size=BATCH, shuffle=True)\n",
    "        # deep-copy so each user starts from the same global weights\n",
    "        w, loss = local_train(copy.deepcopy(global_model), loader,\n",
    "                              lr=LR, epochs=LOCAL_EPOCHS)\n",
    "        local_weights.append(w)\n",
    "        client_losses.append(loss)\n",
    "\n",
    "    # FedAvg\n",
    "    global_model.load_state_dict(average_weights(local_weights, fractions))\n",
    "\n",
    "    mean_loss = np.mean(client_losses)\n",
    "    mean_dice = dice3(global_model, test_dataset)\n",
    "    print(f\"Round {rnd:02d}:  mean-loss = {mean_loss:.4f}   mean-Dice = {mean_dice:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
